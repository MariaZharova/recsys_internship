{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluate_SASRec_BIG_dataset_withEmb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNs1OfcOZZmsIA/aqKMdxoE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaZharova/test_rec_systems/blob/main/Evaluate_SASRec_BIG_dataset_withEmb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fVbsMNBMSl5"
      },
      "outputs": [],
      "source": [
        "! pip install recommenders scrapbook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "from scipy.sparse import csr_matrix, load_npz\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.datasets.split_utils import filter_k_core\n",
        "\n",
        "# Transformer Based Models\n",
        "#from recommenders.models.sasrec.model import SASREC\n",
        "\n",
        "# Sampler for sequential prediction\n",
        "from recommenders.models.sasrec.sampler import WarpSampler\n",
        "from recommenders.models.sasrec.util import SASRecDataSet"
      ],
      "metadata": {
        "id": "p26MJqHcMtdX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from recommenders.models.sasrec.model import Encoder\n",
        "from recommenders.models.sasrec.model import LayerNormalization"
      ],
      "metadata": {
        "id": "C-3WltbzNBxG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afg58BxkZN1X",
        "outputId": "c25fb0de-a0ad-4f88-eb40-b82563395615"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SASREC(tf.keras.Model):\n",
        "    \"\"\"SAS Rec model\n",
        "    Self-Attentive Sequential Recommendation Using Transformer\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        Wang-Cheng Kang, Julian McAuley (2018), Self-Attentive Sequential\n",
        "        Recommendation. Proceedings of IEEE International Conference on\n",
        "        Data Mining (ICDM'18)\n",
        "\n",
        "        Original source code from nnkkmto/SASRec-tf2,\n",
        "        https://github.com/nnkkmto/SASRec-tf2\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"Model initialization.\n",
        "\n",
        "        Args:\n",
        "            item_num (int): Number of items in the dataset.\n",
        "            seq_max_len (int): Maximum number of items in user history.\n",
        "            num_blocks (int): Number of Transformer blocks to be used.\n",
        "            embedding_dim (int): Item embedding dimension.\n",
        "            attention_dim (int): Transformer attention dimension.\n",
        "            conv_dims (list): List of the dimensions of the Feedforward layer.\n",
        "            dropout_rate (float): Dropout rate.\n",
        "            l2_reg (float): Coefficient of the L2 regularization.\n",
        "            num_neg_test (int): Number of negative examples used in testing.\n",
        "        \"\"\"\n",
        "        super(SASREC, self).__init__()\n",
        "\n",
        "        self.item_num = kwargs.get(\"item_num\", None)\n",
        "        self.seq_max_len = kwargs.get(\"seq_max_len\", 100)\n",
        "        self.num_blocks = kwargs.get(\"num_blocks\", 2)\n",
        "        self.embedding_dim = kwargs.get(\"embedding_dim\", 100)#### и сделать emb_dim pos\n",
        "        self.attention_dim = kwargs.get(\"attention_dim\", 100)\n",
        "        self.attention_num_heads = kwargs.get(\"attention_num_heads\", 1)\n",
        "        self.conv_dims = kwargs.get(\"conv_dims\", [100, 100])\n",
        "        self.dropout_rate = kwargs.get(\"dropout_rate\", 0.5)\n",
        "        self.l2_reg = kwargs.get(\"l2_reg\", 0.0)\n",
        "        self.num_neg_test = kwargs.get(\"num_neg_test\", 100)\n",
        "        self.embeddings = kwargs.get(\"embeddings\", None)\n",
        "\n",
        "        #vocab = self.embedding[:, 0]\n",
        "        embd = self.embeddings\n",
        "\n",
        "        #vocab_size = vocab.shape[0]\n",
        "        embedding_dim = embd.shape[0]\n",
        "        embedding = np.asarray(embd)\n",
        "        #del vocab\n",
        "        del embd\n",
        "        embedding = np.vstack((np.zeros((embedding.shape[1])), embedding))\n",
        "        \n",
        "        self.item_embedding_layer = tf.keras.layers.Embedding(\n",
        "            embedding.shape[0],\n",
        "            embedding.shape[1],\n",
        "            name=\"item_embeddings\",\n",
        "            mask_zero=True,\n",
        "            embeddings_regularizer=tf.keras.regularizers.L2(self.l2_reg),\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(embedding.astype(np.float32)), trainable=False)\n",
        "            # индекс строки - кодированные id_offer - Закодировать \n",
        "            # добавить нулевую строку\n",
        "\n",
        "        self.item_embedding_dense_layer = tf.keras.layers.Dense(100, activation='relu')\n",
        "        #print(type(self.item_embedding_layer))\n",
        "        #print(self.item_embedding_layer.shape)\n",
        "\n",
        "        self.positional_embedding_layer = tf.keras.layers.Embedding(\n",
        "            self.seq_max_len,\n",
        "            self.embedding_dim,\n",
        "            name=\"positional_embeddings\",\n",
        "            mask_zero=False,\n",
        "            embeddings_regularizer=tf.keras.regularizers.L2(self.l2_reg),\n",
        "        )\n",
        "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "        self.encoder = Encoder(\n",
        "            self.num_blocks,\n",
        "            self.seq_max_len,\n",
        "            self.embedding_dim,\n",
        "            self.attention_dim,\n",
        "            self.attention_num_heads,\n",
        "            self.conv_dims,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "        self.mask_layer = tf.keras.layers.Masking(mask_value=0)\n",
        "        self.layer_normalization = LayerNormalization(\n",
        "            self.seq_max_len, self.embedding_dim, 1e-08\n",
        "        )\n",
        "\n",
        "    def embedding(self, input_seq):\n",
        "        \"\"\"Compute the sequence and positional embeddings.\n",
        "\n",
        "        Args:\n",
        "            input_seq (tf.Tensor): Input sequence\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor:\n",
        "            - Sequence embeddings.\n",
        "            - Positional embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "        seq_embeddings = self.item_embedding_dense_layer(self.item_embedding_layer(input_seq))\n",
        "        seq_embeddings = seq_embeddings * (self.embedding_dim ** 0.5)\n",
        "\n",
        "        # FIXME\n",
        "        positional_seq = tf.expand_dims(tf.range(tf.shape(input_seq)[1]), 0)\n",
        "        positional_seq = tf.tile(positional_seq, [tf.shape(input_seq)[0], 1])\n",
        "        positional_embeddings = self.positional_embedding_layer(positional_seq)\n",
        "\n",
        "        return seq_embeddings, positional_embeddings\n",
        "\n",
        "    def call(self, x, training):\n",
        "        \"\"\"Model forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor.\n",
        "            training (tf.Tensor): Training tensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor, tf.Tensor:\n",
        "            - Logits of the positive examples.\n",
        "            - Logits of the negative examples.\n",
        "            - Mask for nonzero targets\n",
        "        \"\"\"\n",
        "\n",
        "        input_seq = x[\"input_seq\"]\n",
        "        pos = x[\"positive\"]\n",
        "        neg = x[\"negative\"]\n",
        "\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(input_seq, 0), tf.float32), -1)\n",
        "        seq_embeddings, positional_embeddings = self.embedding(input_seq)\n",
        "\n",
        "        # add positional embeddings\n",
        "        seq_embeddings += positional_embeddings\n",
        "\n",
        "        # dropout\n",
        "        seq_embeddings = self.dropout_layer(seq_embeddings)\n",
        "\n",
        "        # masking\n",
        "        seq_embeddings *= mask\n",
        "\n",
        "        # --- ATTENTION BLOCKS ---\n",
        "        seq_attention = seq_embeddings\n",
        "        seq_attention = self.encoder(seq_attention, training, mask)\n",
        "        seq_attention = self.layer_normalization(seq_attention)  # (b, s, d)\n",
        "\n",
        "        # --- PREDICTION LAYER ---\n",
        "        # user's sequence embedding\n",
        "        pos = self.mask_layer(pos)\n",
        "        neg = self.mask_layer(neg)\n",
        "\n",
        "        pos = tf.reshape(pos, [tf.shape(input_seq)[0] * self.seq_max_len])\n",
        "        neg = tf.reshape(neg, [tf.shape(input_seq)[0] * self.seq_max_len])\n",
        "        pos_emb = self.item_embedding_dense_layer(self.item_embedding_layer(pos))\n",
        "        neg_emb = self.item_embedding_dense_layer(self.item_embedding_layer(neg))\n",
        "        seq_emb = tf.reshape(\n",
        "            seq_attention,\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len, self.embedding_dim],\n",
        "        )  # (b*s, d)\n",
        "\n",
        "        pos_logits = tf.reduce_sum(pos_emb * seq_emb, -1)\n",
        "        neg_logits = tf.reduce_sum(neg_emb * seq_emb, -1)\n",
        "\n",
        "        pos_logits = tf.expand_dims(pos_logits, axis=-1)  # (bs, 1)\n",
        "        # pos_prob = tf.keras.layers.Dense(1, activation='sigmoid')(pos_logits)  # (bs, 1)\n",
        "        # 100, relu\n",
        "\n",
        "        neg_logits = tf.expand_dims(neg_logits, axis=-1)  # (bs, 1)\n",
        "        # neg_prob = tf.keras.layers.Dense(1, activation='sigmoid')(neg_logits)  # (bs, 1)\n",
        "\n",
        "        # output = tf.concat([pos_logits, neg_logits], axis=0)\n",
        "\n",
        "        # masking for loss calculation\n",
        "        istarget = tf.reshape(\n",
        "            tf.cast(tf.not_equal(pos, 0), dtype=tf.float32),\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len],\n",
        "        )\n",
        "\n",
        "        return pos_logits, neg_logits, istarget\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"Returns the logits for the test items.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "             tf.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        training = False\n",
        "        input_seq = inputs[\"input_seq\"]\n",
        "        candidate = inputs[\"candidate\"]\n",
        "\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(input_seq, 0), tf.float32), -1)\n",
        "        seq_embeddings, positional_embeddings = self.embedding(input_seq)\n",
        "        seq_embeddings += positional_embeddings\n",
        "        # seq_embeddings = self.dropout_layer(seq_embeddings)\n",
        "        seq_embeddings *= mask\n",
        "        seq_attention = seq_embeddings\n",
        "        seq_attention = self.encoder(seq_attention, training, mask)\n",
        "        seq_attention = self.layer_normalization(seq_attention)  # (b, s, d)\n",
        "        seq_emb = tf.reshape(\n",
        "            seq_attention,\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len, self.embedding_dim],\n",
        "        )  # (b*s, d)\n",
        "        candidate_emb = self.item_embedding_dense_layer(self.item_embedding_layer(candidate))  # (b, s, d)\n",
        "        candidate_emb = tf.transpose(candidate_emb, perm=[0, 2, 1])  # (b, d, s)\n",
        "        \n",
        "        test_logits = tf.matmul(seq_emb, candidate_emb)\n",
        "        # (200, 100) * (1, 101, 100)'\n",
        "\n",
        "        test_logits = tf.reshape(\n",
        "            test_logits,\n",
        "            [tf.shape(input_seq)[0], self.seq_max_len, 1 + self.num_neg_test],\n",
        "        )  # (1, 200, 101)\n",
        "        test_logits = test_logits[:, -1, :]  # (1, 101)\n",
        "        return test_logits\n",
        "\n",
        "    def loss_function(self, pos_logits, neg_logits, istarget):\n",
        "        \"\"\"Losses are calculated separately for the positive and negative\n",
        "        items based on the corresponding logits. A mask is included to\n",
        "        take care of the zero items (added for padding).\n",
        "\n",
        "        Args:\n",
        "            pos_logits (tf.Tensor): Logits of the positive examples.\n",
        "            neg_logits (tf.Tensor): Logits of the negative examples.\n",
        "            istarget (tf.Tensor): Mask for nonzero targets.\n",
        "\n",
        "        Returns:\n",
        "            float: Loss.\n",
        "        \"\"\"\n",
        "\n",
        "        pos_logits = pos_logits[:, 0]\n",
        "        neg_logits = neg_logits[:, 0]\n",
        "\n",
        "        # ignore padding items (0)\n",
        "        # istarget = tf.reshape(\n",
        "        #     tf.cast(tf.not_equal(self.pos, 0), dtype=tf.float32),\n",
        "        #     [tf.shape(self.input_seq)[0] * self.seq_max_len],\n",
        "        # )\n",
        "        # for logits\n",
        "        loss = tf.reduce_sum(\n",
        "            -tf.math.log(tf.math.sigmoid(pos_logits) + 1e-24) * istarget\n",
        "            - tf.math.log(1 - tf.math.sigmoid(neg_logits) + 1e-24) * istarget\n",
        "        ) / tf.reduce_sum(istarget)\n",
        "\n",
        "        # for probabilities\n",
        "        # loss = tf.reduce_sum(\n",
        "        #         - tf.math.log(pos_logits + 1e-24) * istarget -\n",
        "        #         tf.math.log(1 - neg_logits + 1e-24) * istarget\n",
        "        # ) / tf.reduce_sum(istarget)\n",
        "        reg_loss = tf.compat.v1.losses.get_regularization_loss()\n",
        "        # reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n",
        "        # loss += sum(reg_losses)\n",
        "        loss += reg_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def create_combined_dataset(self, u, seq, pos, neg):\n",
        "        \"\"\"\n",
        "        function to create model inputs from sampled batch data.\n",
        "        This function is used only during training.\n",
        "        \"\"\"\n",
        "        inputs = {}\n",
        "        seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            seq, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "        pos = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            pos, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "        neg = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            neg, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "\n",
        "        inputs[\"users\"] = np.expand_dims(np.array(u), axis=-1)\n",
        "        inputs[\"input_seq\"] = seq\n",
        "        inputs[\"positive\"] = pos\n",
        "        inputs[\"negative\"] = neg\n",
        "\n",
        "        target = np.concatenate(\n",
        "            [\n",
        "                np.repeat(1, seq.shape[0] * seq.shape[1]),\n",
        "                np.repeat(0, seq.shape[0] * seq.shape[1]),\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        target = np.expand_dims(target, axis=-1)\n",
        "        return inputs, target\n",
        "\n",
        "    def train(self, dataset, sampler, **kwargs):\n",
        "        \"\"\"\n",
        "        High level function for model training as well as\n",
        "        evaluation on the validation and test dataset\n",
        "        \"\"\"\n",
        "\n",
        "        num_epochs = kwargs.get(\"num_epochs\", 10)\n",
        "        batch_size = kwargs.get(\"batch_size\", 128)\n",
        "        lr = kwargs.get(\"learning_rate\", 0.001)\n",
        "        val_epoch = kwargs.get(\"val_epoch\", 5)\n",
        "\n",
        "        num_steps = int(len(dataset.user_train) / batch_size)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-7\n",
        "        )\n",
        "\n",
        "        loss_function = self.loss_function\n",
        "\n",
        "        train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "        train_step_signature = [\n",
        "            {\n",
        "                \"users\": tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "                \"input_seq\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "                \"positive\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "                \"negative\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "            },\n",
        "            tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "        ]\n",
        "\n",
        "        @tf.function(input_signature=train_step_signature)\n",
        "        def train_step(inp, tar):\n",
        "            with tf.GradientTape() as tape:\n",
        "                pos_logits, neg_logits, loss_mask = self(inp, training=True)\n",
        "                loss = loss_function(pos_logits, neg_logits, loss_mask)\n",
        "\n",
        "            gradients = tape.gradient(loss, self.trainable_variables)\n",
        "            gradients = [(tf.clip_by_value(grad, -1.0, 1.0)) for grad in gradients]\n",
        "            optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "            train_loss(loss)\n",
        "            return loss\n",
        "\n",
        "        T = 0.0\n",
        "        t0 = Timer()\n",
        "        t0.start()\n",
        "\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "            step_loss = []\n",
        "            train_loss.reset_states()\n",
        "            for step in tqdm(\n",
        "                range(num_steps), total=num_steps, ncols=70, leave=False, unit=\"b\"\n",
        "            ):\n",
        "\n",
        "                u, seq, pos, neg = sampler.next_batch()\n",
        "\n",
        "                inputs, target = self.create_combined_dataset(u, seq, pos, neg)\n",
        "\n",
        "                loss = train_step(inputs, target)\n",
        "                step_loss.append(loss)\n",
        "\n",
        "            if epoch % val_epoch == 0:\n",
        "                t0.stop()\n",
        "                t1 = t0.interval\n",
        "                T += t1\n",
        "                print(\"Evaluating...\")\n",
        "                t_test = self.evaluate(dataset)\n",
        "                t_valid = self.evaluate_valid(dataset)\n",
        "                print(\n",
        "                    f\"\\nepoch: {epoch}, time: {T}, valid (NDCG@10: {t_valid[0]}, HR@10: {t_valid[1]})\"\n",
        "                )\n",
        "                print(\n",
        "                    f\"epoch: {epoch}, time: {T},  test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\"\n",
        "                )\n",
        "                t0.start()\n",
        "\n",
        "        t_test = self.evaluate(dataset)\n",
        "        print(f\"\\nepoch: {epoch}, test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\")\n",
        "\n",
        "        return t_test\n",
        "\n",
        "    def evaluate(self, dataset):\n",
        "        \"\"\"\n",
        "        Evaluation on the test users (users with at least 3 items)\n",
        "        \"\"\"\n",
        "        usernum = dataset.usernum\n",
        "        itemnum = dataset.itemnum\n",
        "        train = dataset.user_train  # removing deepcopy\n",
        "        valid = dataset.user_valid\n",
        "        test = dataset.user_test\n",
        "\n",
        "        NDCG = 0.0\n",
        "        HT = 0.0\n",
        "        valid_user = 0.0\n",
        "\n",
        "        if usernum > 1000:\n",
        "            users = random.sample(range(1, usernum + 1), 1000)\n",
        "        else:\n",
        "            users = range(1, usernum + 1)\n",
        "\n",
        "        for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "\n",
        "            if len(train[u]) < 1 or len(test[u]) < 1:\n",
        "                continue\n",
        "\n",
        "            seq = np.zeros([self.seq_max_len], dtype=np.int32)\n",
        "            idx = self.seq_max_len - 1\n",
        "            seq[idx] = valid[u][0]\n",
        "            idx -= 1\n",
        "            for i in reversed(train[u]):\n",
        "                seq[idx] = i\n",
        "                idx -= 1\n",
        "                if idx == -1:\n",
        "                    break\n",
        "            rated = set(train[u])\n",
        "            rated.add(0)\n",
        "            item_idx = [test[u][0]]\n",
        "            for _ in range(self.num_neg_test):\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "                while t in rated:\n",
        "                    t = np.random.randint(1, itemnum + 1)\n",
        "                item_idx.append(t)\n",
        "\n",
        "            inputs = {}\n",
        "            inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "            inputs[\"input_seq\"] = np.array([seq])\n",
        "            inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "            # inverse to get descending sort\n",
        "            predictions = -1.0 * self.predict(inputs)\n",
        "            predictions = np.array(predictions)\n",
        "            predictions = predictions[0]\n",
        "\n",
        "            rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "            valid_user += 1\n",
        "\n",
        "            if rank < 10:\n",
        "                NDCG += 1 / np.log2(rank + 2)\n",
        "                HT += 1\n",
        "\n",
        "        return NDCG / valid_user, HT / valid_user\n",
        "\n",
        "    def evaluate_valid(self, dataset):\n",
        "        \"\"\"\n",
        "        Evaluation on the validation users\n",
        "        \"\"\"\n",
        "        usernum = dataset.usernum\n",
        "        itemnum = dataset.itemnum\n",
        "        train = dataset.user_train  # removing deepcopy\n",
        "        valid = dataset.user_valid\n",
        "\n",
        "        NDCG = 0.0\n",
        "        valid_user = 0.0\n",
        "        HT = 0.0\n",
        "        if usernum > 1000:\n",
        "            users = random.sample(range(1, usernum + 1), 1000)\n",
        "        else:\n",
        "            users = range(1, usernum + 1)\n",
        "\n",
        "        for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "            if len(train[u]) < 1 or len(valid[u]) < 1:\n",
        "                continue\n",
        "\n",
        "            seq = np.zeros([self.seq_max_len], dtype=np.int32)\n",
        "            idx = self.seq_max_len - 1\n",
        "            for i in reversed(train[u]):\n",
        "                seq[idx] = i\n",
        "                idx -= 1\n",
        "                if idx == -1:\n",
        "                    break\n",
        "\n",
        "            rated = set(train[u])\n",
        "            rated.add(0)\n",
        "            item_idx = [valid[u][0]]\n",
        "            for _ in range(self.num_neg_test):\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "                while t in rated:\n",
        "                    t = np.random.randint(1, itemnum + 1)\n",
        "                item_idx.append(t)\n",
        "\n",
        "            inputs = {}\n",
        "            inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "            inputs[\"input_seq\"] = np.array([seq])\n",
        "            inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "            # predictions = -model.predict(sess, [u], [seq], item_idx)\n",
        "            predictions = -1.0 * self.predict(inputs)\n",
        "            predictions = np.array(predictions)\n",
        "            predictions = predictions[0]\n",
        "\n",
        "            rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "            valid_user += 1\n",
        "\n",
        "            if rank < 10:\n",
        "                NDCG += 1 / np.log2(rank + 2)\n",
        "                HT += 1\n",
        "\n",
        "        return NDCG / valid_user, HT / valid_user\n"
      ],
      "metadata": {
        "id": "NjxIm3a5NErQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create special data format for SAS\n",
        "dataS = SASRecDataSet(filename='out_e.txt', col_sep='\\t')\n",
        "# split into train, test and validation\n",
        "dataS.split()"
      ],
      "metadata": {
        "id": "y1FsbG2qN-Lj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = np.load('embeddings_filtered.npy')"
      ],
      "metadata": {
        "id": "zP-PrgwxZ00B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model variables\n",
        "num_epochs = 40\n",
        "batch_size = 80    #                                                         !!!\n",
        "RANDOM_SEED = 100  # Set None for non-deterministic result\n",
        "lr = 0.0001             # learning rate                                      !!!\n",
        "maxlen = 50            # maximum sequence length for each user\n",
        "num_blocks = 2         # number of transformer blocks\n",
        "hidden_units = 50      # number of units in the attention calculation.       !!!\n",
        "num_heads = 1          # number of attention heads\n",
        "dropout_rate = 0.1     # dropout rate\n",
        "l2_emb = 0.05          # L2 regularization coefficient                       !!!\n",
        "num_neg_test = 1252955 # number of negative examples per positive example"
      ],
      "metadata": {
        "id": "HxFR84BuZnSo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample negative examples\n",
        "sampler = WarpSampler(dataS.user_train, dataS.usernum, dataS.itemnum, batch_size=batch_size, maxlen=maxlen, n_workers=3)"
      ],
      "metadata": {
        "id": "aCQrpIIPZnVh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SASREC(item_num=dataS.itemnum,\n",
        "               seq_max_len=maxlen,\n",
        "               num_blocks=num_blocks,\n",
        "               embedding_dim=hidden_units,\n",
        "               attention_dim=hidden_units,\n",
        "               attention_num_heads=num_heads,\n",
        "               dropout_rate=dropout_rate,\n",
        "               conv_dims = [50, 50],\n",
        "               l2_reg=l2_emb,\n",
        "               num_neg_test=num_neg_test,\n",
        "               embeddings=embeddings[:, 1:])"
      ],
      "metadata": {
        "id": "aVpRpYZYZnXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with Timer() as train_time:\n",
        "    t_test = model.train(dataS, sampler, num_epochs=num_epochs, batch_size=batch_size, lr=lr, val_epoch=5)"
      ],
      "metadata": {
        "id": "T2Qvw6RbN-S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JWWZCoqDN-X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1SD9uxLoN-aS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}