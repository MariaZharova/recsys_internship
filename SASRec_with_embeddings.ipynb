{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaZharova/test_rec_systems/blob/main/SASRec_with_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_8zOYqp3D7yA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e21d0b6a-022d-4ceb-eb7e-38cf0bb15ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting recommenders\n",
            "  Downloading recommenders-1.1.1-py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 29.1 MB/s \n",
            "\u001b[?25hCollecting scrapbook\n",
            "  Downloading scrapbook-0.5.0-py3-none-any.whl (34 kB)\n",
            "Collecting scikit-surprise>=1.0.6\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2<3.1,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.11.3)\n",
            "Collecting category-encoders<2,>=1.3.0\n",
            "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.11.2)\n",
            "Collecting memory-profiler<1,>=0.54.0\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Collecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Collecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 26.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.21.6)\n",
            "Collecting bottleneck<2,>=1.2.1\n",
            "  Downloading Bottleneck-1.3.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[K     |████████████████████████████████| 355 kB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.23.0)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.7.3)\n",
            "Requirement already satisfied: nltk<4,>=3.4 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.7)\n",
            "Requirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.64.0)\n",
            "Collecting cornac<2,>=1.1.2\n",
            "  Downloading cornac-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 27.1 MB/s \n",
            "\u001b[?25hCollecting pandera[strategies]>=0.6.5\n",
            "  Downloading pandera-0.9.0-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.2.2)\n",
            "Requirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.51.2)\n",
            "Requirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.2.3)\n",
            "Collecting transformers<5,>=2.5.0\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 41.6 MB/s \n",
            "\u001b[?25hCollecting lightfm<2,>=1.15\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.5)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.2)\n",
            "Collecting powerlaw\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3.1,>=2->recommenders) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4,>=2.2.2->recommenders) (4.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.4.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders) (2022.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (6.0.1)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.9.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (21.3)\n",
            "Collecting hypothesis>=5.41.1\n",
            "  Downloading hypothesis-6.53.0-py3-none-any.whl (389 kB)\n",
            "\u001b[K     |████████████████████████████████| 389 kB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (21.4.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.0rc8-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (3.7.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (4.12.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from scrapbook) (4.3.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from scrapbook) (5.5.0)\n",
            "Collecting papermill\n",
            "  Downloading papermill-2.3.4-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders) (3.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->scrapbook) (0.2.5)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (5.8.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.4)\n",
            "Collecting ansiwrap\n",
            "  Downloading ansiwrap-0.8.4-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.6.6)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (8.0.1)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (5.4.0)\n",
            "Collecting jupyter-client>=6.1.5\n",
            "  Downloading jupyter_client-7.3.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 16.5 MB/s \n",
            "\u001b[?25hCollecting traitlets>=4.2\n",
            "  Downloading traitlets-5.3.0-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (1.5.5)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (4.11.1)\n",
            "Requirement already satisfied: pyzmq>=23.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (23.2.0)\n",
            "Collecting tornado>=6.0\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[K     |████████████████████████████████| 423 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (2.16.1)\n",
            "Collecting textwrap3>=0.9.2\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->scrapbook) (0.7.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.2.1)\n",
            "Building wheels for collected packages: lightfm, memory-profiler, retrying, scikit-surprise\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705376 sha256=e087088af3cada6e86004a405fbe7654764cd39dac195b4216504a22639e6d8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=eb6266959ae82531679e44294d5384cc003fc799bfbe430e0f383bc8fe0468e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=ef7450962450d05455dfb953fd9bd84f1b52725390383fdcbdd906d0fdb61606\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633715 sha256=f31fbaf07ba4f61c86602c9516b32b7fbadbbaf7db9d68e04df16d8b19b90e5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built lightfm memory-profiler retrying scikit-surprise\n",
            "Installing collected packages: traitlets, tornado, mypy-extensions, typing-inspect, textwrap3, pyyaml, jupyter-client, exceptiongroup, tokenizers, powerlaw, pandera, hypothesis, huggingface-hub, ansiwrap, transformers, scikit-surprise, retrying, papermill, memory-profiler, lightfm, cornac, category-encoders, bottleneck, scrapbook, recommenders\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.2 which is incompatible.\u001b[0m\n",
            "Successfully installed ansiwrap-0.8.4 bottleneck-1.3.5 category-encoders-1.3.0 cornac-1.14.2 exceptiongroup-1.0.0rc8 huggingface-hub-0.8.1 hypothesis-6.53.0 jupyter-client-7.3.4 lightfm-1.16 memory-profiler-0.60.0 mypy-extensions-0.4.3 pandera-0.9.0 papermill-2.3.4 powerlaw-1.5 pyyaml-5.4.1 recommenders-1.1.1 retrying-1.3.3 scikit-surprise-1.1.1 scrapbook-0.5.0 textwrap3-0.9.2 tokenizers-0.12.1 tornado-6.2 traitlets-5.3.0 transformers-4.20.1 typing-inspect-0.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install recommenders scrapbook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oEcypBYgz0Eu"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "from scipy.sparse import csr_matrix, load_npz\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.datasets.split_utils import filter_k_core\n",
        "\n",
        "# Transformer Based Models\n",
        "#from recommenders.models.sasrec.model import SASREC\n",
        "\n",
        "# Sampler for sequential prediction\n",
        "from recommenders.models.sasrec.sampler import WarpSampler\n",
        "from recommenders.models.sasrec.util import SASRecDataSet\n",
        "\n",
        "# Evaluation\n",
        "from recommenders.evaluation.python_evaluation import precision_at_k"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from recommenders.utils.timer import Timer\n",
        "#from recommenders.models.sasrec.model import MultiHeadAttention\n",
        "#from recommenders.models.sasrec.model import PointWiseFeedForward\n",
        "#from recommenders.models.sasrec.model import EncoderLayer\n",
        "from recommenders.models.sasrec.model import Encoder\n",
        "from recommenders.models.sasrec.model import LayerNormalization"
      ],
      "metadata": {
        "id": "TyD8MKGVWy4w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltWaoZ3JTqX0",
        "outputId": "4f06950f-af28-4eda-99ca-0acaa06ac070"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Changed SASRec code"
      ],
      "metadata": {
        "id": "NBnLymgJRTji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SASREC(tf.keras.Model):\n",
        "    \"\"\"SAS Rec model\n",
        "    Self-Attentive Sequential Recommendation Using Transformer\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        Wang-Cheng Kang, Julian McAuley (2018), Self-Attentive Sequential\n",
        "        Recommendation. Proceedings of IEEE International Conference on\n",
        "        Data Mining (ICDM'18)\n",
        "\n",
        "        Original source code from nnkkmto/SASRec-tf2,\n",
        "        https://github.com/nnkkmto/SASRec-tf2\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"Model initialization.\n",
        "\n",
        "        Args:\n",
        "            item_num (int): Number of items in the dataset.\n",
        "            seq_max_len (int): Maximum number of items in user history.\n",
        "            num_blocks (int): Number of Transformer blocks to be used.\n",
        "            embedding_dim (int): Item embedding dimension.\n",
        "            attention_dim (int): Transformer attention dimension.\n",
        "            conv_dims (list): List of the dimensions of the Feedforward layer.\n",
        "            dropout_rate (float): Dropout rate.\n",
        "            l2_reg (float): Coefficient of the L2 regularization.\n",
        "            num_neg_test (int): Number of negative examples used in testing.\n",
        "        \"\"\"\n",
        "        super(SASREC, self).__init__()\n",
        "\n",
        "        self.item_num = kwargs.get(\"item_num\", None)\n",
        "        self.seq_max_len = kwargs.get(\"seq_max_len\", 100)\n",
        "        self.num_blocks = kwargs.get(\"num_blocks\", 2)\n",
        "        self.embedding_dim = kwargs.get(\"embedding_dim\", 100)#### и сделать emb_dim pos\n",
        "        self.attention_dim = kwargs.get(\"attention_dim\", 100)\n",
        "        self.attention_num_heads = kwargs.get(\"attention_num_heads\", 1)\n",
        "        self.conv_dims = kwargs.get(\"conv_dims\", [100, 100])\n",
        "        self.dropout_rate = kwargs.get(\"dropout_rate\", 0.5)\n",
        "        self.l2_reg = kwargs.get(\"l2_reg\", 0.0)\n",
        "        self.num_neg_test = kwargs.get(\"num_neg_test\", 100)\n",
        "\n",
        "        ####======== READ FILE\n",
        "        file = np.load('./drive/MyDrive/ML/embeddings_test10000.npy')\n",
        "\n",
        "        vocab = file[:, 0]\n",
        "        embd = file[:, 1:]\n",
        "\n",
        "        vocab_size = vocab.shape[0]\n",
        "        embedding_dim = embd.shape[0]\n",
        "        embedding = np.asarray(embd)\n",
        "        del vocab\n",
        "        del embd\n",
        "        embedding = np.vstack((np.zeros((embedding.shape[1])), embedding)) \n",
        "        \n",
        "        self.item_embedding_layer = tf.keras.layers.Embedding(\n",
        "            embedding.shape[0],\n",
        "            embedding.shape[1],\n",
        "            name=\"item_embeddings\",\n",
        "            mask_zero=True,\n",
        "            embeddings_regularizer=tf.keras.regularizers.L2(self.l2_reg),\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(embedding.astype(np.float32)), trainable=False)\n",
        "            # индекс строки - кодированные id_offer - Закодировать \n",
        "            # добавить нулевую строку\n",
        "\n",
        "        self.item_embedding_dense_layer = tf.keras.layers.Dense(100, activation='relu')\n",
        "        #print(type(self.item_embedding_layer))\n",
        "        #print(self.item_embedding_layer.shape)\n",
        "\n",
        "        self.positional_embedding_layer = tf.keras.layers.Embedding(\n",
        "            self.seq_max_len,\n",
        "            self.embedding_dim,\n",
        "            name=\"positional_embeddings\",\n",
        "            mask_zero=False,\n",
        "            embeddings_regularizer=tf.keras.regularizers.L2(self.l2_reg),\n",
        "        )\n",
        "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "        self.encoder = Encoder(\n",
        "            self.num_blocks,\n",
        "            self.seq_max_len,\n",
        "            self.embedding_dim,\n",
        "            self.attention_dim,\n",
        "            self.attention_num_heads,\n",
        "            self.conv_dims,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "        self.mask_layer = tf.keras.layers.Masking(mask_value=0)\n",
        "        self.layer_normalization = LayerNormalization(\n",
        "            self.seq_max_len, self.embedding_dim, 1e-08\n",
        "        )\n",
        "\n",
        "    def embedding(self, input_seq):\n",
        "        \"\"\"Compute the sequence and positional embeddings.\n",
        "\n",
        "        Args:\n",
        "            input_seq (tf.Tensor): Input sequence\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor:\n",
        "            - Sequence embeddings.\n",
        "            - Positional embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "        seq_embeddings = self.item_embedding_dense_layer(self.item_embedding_layer(input_seq))\n",
        "        seq_embeddings = seq_embeddings * (self.embedding_dim ** 0.5)\n",
        "\n",
        "        # FIXME\n",
        "        positional_seq = tf.expand_dims(tf.range(tf.shape(input_seq)[1]), 0)\n",
        "        positional_seq = tf.tile(positional_seq, [tf.shape(input_seq)[0], 1])\n",
        "        positional_embeddings = self.positional_embedding_layer(positional_seq)\n",
        "\n",
        "        return seq_embeddings, positional_embeddings\n",
        "\n",
        "    def call(self, x, training):\n",
        "        \"\"\"Model forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor.\n",
        "            training (tf.Tensor): Training tensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor, tf.Tensor:\n",
        "            - Logits of the positive examples.\n",
        "            - Logits of the negative examples.\n",
        "            - Mask for nonzero targets\n",
        "        \"\"\"\n",
        "\n",
        "        input_seq = x[\"input_seq\"]\n",
        "        pos = x[\"positive\"]\n",
        "        neg = x[\"negative\"]\n",
        "\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(input_seq, 0), tf.float32), -1)\n",
        "        seq_embeddings, positional_embeddings = self.embedding(input_seq)\n",
        "        #print(tf.shape(seq_embeddings), tf.shape(positional_embeddings))\n",
        "\n",
        "        # add positional embeddings\n",
        "        seq_embeddings += positional_embeddings\n",
        "\n",
        "        #print('PPP1')\n",
        "\n",
        "        # dropout\n",
        "        seq_embeddings = self.dropout_layer(seq_embeddings)\n",
        "\n",
        "        # masking\n",
        "        seq_embeddings *= mask\n",
        "\n",
        "        # --- ATTENTION BLOCKS ---\n",
        "        seq_attention = seq_embeddings\n",
        "        seq_attention = self.encoder(seq_attention, training, mask)\n",
        "        seq_attention = self.layer_normalization(seq_attention)  # (b, s, d)\n",
        "\n",
        "        # --- PREDICTION LAYER ---\n",
        "        # user's sequence embedding\n",
        "        pos = self.mask_layer(pos)\n",
        "        neg = self.mask_layer(neg)\n",
        "\n",
        "        #print('PPP2')\n",
        "\n",
        "        pos = tf.reshape(pos, [tf.shape(input_seq)[0] * self.seq_max_len])\n",
        "        neg = tf.reshape(neg, [tf.shape(input_seq)[0] * self.seq_max_len])\n",
        "        pos_emb = self.item_embedding_dense_layer(self.item_embedding_layer(pos))\n",
        "        neg_emb = self.item_embedding_dense_layer(self.item_embedding_layer(neg))\n",
        "        seq_emb = tf.reshape(\n",
        "            seq_attention,\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len, self.embedding_dim],\n",
        "        )  # (b*s, d)\n",
        "\n",
        "\n",
        "        #print('PPP3')\n",
        "        pos_logits = tf.reduce_sum(pos_emb * seq_emb, -1)\n",
        "        neg_logits = tf.reduce_sum(neg_emb * seq_emb, -1)\n",
        "\n",
        "        #print('PPP4')\n",
        "\n",
        "        pos_logits = tf.expand_dims(pos_logits, axis=-1)  # (bs, 1)\n",
        "        # pos_prob = tf.keras.layers.Dense(1, activation='sigmoid')(pos_logits)  # (bs, 1)\n",
        "        # 100, relu\n",
        "\n",
        "        neg_logits = tf.expand_dims(neg_logits, axis=-1)  # (bs, 1)\n",
        "        # neg_prob = tf.keras.layers.Dense(1, activation='sigmoid')(neg_logits)  # (bs, 1)\n",
        "\n",
        "        # output = tf.concat([pos_logits, neg_logits], axis=0)\n",
        "\n",
        "        # masking for loss calculation\n",
        "        istarget = tf.reshape(\n",
        "            tf.cast(tf.not_equal(pos, 0), dtype=tf.float32),\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len],\n",
        "        )\n",
        "\n",
        "        return pos_logits, neg_logits, istarget\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"Returns the logits for the test items.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "             tf.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        training = False\n",
        "        input_seq = inputs[\"input_seq\"]\n",
        "        candidate = inputs[\"candidate\"]\n",
        "\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(input_seq, 0), tf.float32), -1)\n",
        "        seq_embeddings, positional_embeddings = self.embedding(input_seq)\n",
        "        seq_embeddings += positional_embeddings\n",
        "        # seq_embeddings = self.dropout_layer(seq_embeddings)\n",
        "        seq_embeddings *= mask\n",
        "        seq_attention = seq_embeddings\n",
        "        seq_attention = self.encoder(seq_attention, training, mask)\n",
        "        seq_attention = self.layer_normalization(seq_attention)  # (b, s, d)\n",
        "        seq_emb = tf.reshape(\n",
        "            seq_attention,\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len, self.embedding_dim],\n",
        "        )  # (b*s, d)\n",
        "        candidate_emb = self.item_embedding_dense_layer(self.item_embedding_layer(candidate))  # (b, s, d)\n",
        "        candidate_emb = tf.transpose(candidate_emb, perm=[0, 2, 1])  # (b, d, s)\n",
        "        \n",
        "        #print('IN EVALUATE:', tf.shape(seq_emb), tf.shape(candidate_emb))\n",
        "        #print('\\n')\n",
        "        test_logits = tf.matmul(seq_emb, candidate_emb)\n",
        "        # (200, 100) * (1, 101, 100)'\n",
        "\n",
        "        test_logits = tf.reshape(\n",
        "            test_logits,\n",
        "            [tf.shape(input_seq)[0], self.seq_max_len, 1 + self.num_neg_test],\n",
        "        )  # (1, 200, 101)\n",
        "        test_logits = test_logits[:, -1, :]  # (1, 101)\n",
        "        return test_logits\n",
        "\n",
        "    def loss_function(self, pos_logits, neg_logits, istarget):\n",
        "        \"\"\"Losses are calculated separately for the positive and negative\n",
        "        items based on the corresponding logits. A mask is included to\n",
        "        take care of the zero items (added for padding).\n",
        "\n",
        "        Args:\n",
        "            pos_logits (tf.Tensor): Logits of the positive examples.\n",
        "            neg_logits (tf.Tensor): Logits of the negative examples.\n",
        "            istarget (tf.Tensor): Mask for nonzero targets.\n",
        "\n",
        "        Returns:\n",
        "            float: Loss.\n",
        "        \"\"\"\n",
        "\n",
        "        pos_logits = pos_logits[:, 0]\n",
        "        neg_logits = neg_logits[:, 0]\n",
        "\n",
        "        # ignore padding items (0)\n",
        "        # istarget = tf.reshape(\n",
        "        #     tf.cast(tf.not_equal(self.pos, 0), dtype=tf.float32),\n",
        "        #     [tf.shape(self.input_seq)[0] * self.seq_max_len],\n",
        "        # )\n",
        "        # for logits\n",
        "        loss = tf.reduce_sum(\n",
        "            -tf.math.log(tf.math.sigmoid(pos_logits) + 1e-24) * istarget\n",
        "            - tf.math.log(1 - tf.math.sigmoid(neg_logits) + 1e-24) * istarget\n",
        "        ) / tf.reduce_sum(istarget)\n",
        "\n",
        "        # for probabilities\n",
        "        # loss = tf.reduce_sum(\n",
        "        #         - tf.math.log(pos_logits + 1e-24) * istarget -\n",
        "        #         tf.math.log(1 - neg_logits + 1e-24) * istarget\n",
        "        # ) / tf.reduce_sum(istarget)\n",
        "        reg_loss = tf.compat.v1.losses.get_regularization_loss()\n",
        "        # reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n",
        "        # loss += sum(reg_losses)\n",
        "        loss += reg_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def create_combined_dataset(self, u, seq, pos, neg):\n",
        "        \"\"\"\n",
        "        function to create model inputs from sampled batch data.\n",
        "        This function is used only during training.\n",
        "        \"\"\"\n",
        "        inputs = {}\n",
        "        seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            seq, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "        pos = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            pos, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "        neg = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            neg, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "\n",
        "        inputs[\"users\"] = np.expand_dims(np.array(u), axis=-1)\n",
        "        inputs[\"input_seq\"] = seq\n",
        "        inputs[\"positive\"] = pos\n",
        "        inputs[\"negative\"] = neg\n",
        "\n",
        "        target = np.concatenate(\n",
        "            [\n",
        "                np.repeat(1, seq.shape[0] * seq.shape[1]),\n",
        "                np.repeat(0, seq.shape[0] * seq.shape[1]),\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        target = np.expand_dims(target, axis=-1)\n",
        "        return inputs, target\n",
        "\n",
        "    def train(self, dataset, sampler, **kwargs):\n",
        "        \"\"\"\n",
        "        High level function for model training as well as\n",
        "        evaluation on the validation and test dataset\n",
        "        \"\"\"\n",
        "\n",
        "        #print(\"IN TRAIN!\")\n",
        "\n",
        "        num_epochs = kwargs.get(\"num_epochs\", 10)\n",
        "        batch_size = kwargs.get(\"batch_size\", 128)\n",
        "        lr = kwargs.get(\"learning_rate\", 0.001)\n",
        "        val_epoch = kwargs.get(\"val_epoch\", 5)\n",
        "\n",
        "        num_steps = int(len(dataset.user_train) / batch_size)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-7\n",
        "        )\n",
        "\n",
        "        loss_function = self.loss_function\n",
        "\n",
        "        train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "        train_step_signature = [\n",
        "            {\n",
        "                \"users\": tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "                \"input_seq\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "                \"positive\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "                \"negative\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "            },\n",
        "            tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "        ]\n",
        "\n",
        "        @tf.function(input_signature=train_step_signature)\n",
        "        def train_step(inp, tar):\n",
        "            with tf.GradientTape() as tape:\n",
        "                pos_logits, neg_logits, loss_mask = self(inp, training=True)\n",
        "                loss = loss_function(pos_logits, neg_logits, loss_mask)\n",
        "\n",
        "            gradients = tape.gradient(loss, self.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "            train_loss(loss)\n",
        "            return loss\n",
        "\n",
        "        T = 0.0\n",
        "        t0 = Timer()\n",
        "        t0.start()\n",
        "\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "            step_loss = []\n",
        "            train_loss.reset_states()\n",
        "            for step in tqdm(\n",
        "                range(num_steps), total=num_steps, ncols=70, leave=False, unit=\"b\"\n",
        "            ):\n",
        "\n",
        "                u, seq, pos, neg = sampler.next_batch()\n",
        "\n",
        "                inputs, target = self.create_combined_dataset(u, seq, pos, neg)\n",
        "\n",
        "                loss = train_step(inputs, target)\n",
        "                step_loss.append(loss)\n",
        "\n",
        "            if epoch % val_epoch == 0:\n",
        "                t0.stop()\n",
        "                t1 = t0.interval\n",
        "                T += t1\n",
        "                print(\"Evaluating...\")\n",
        "                t_test = self.evaluate(dataset)\n",
        "                t_valid = self.evaluate_valid(dataset)\n",
        "                print(\n",
        "                    f\"\\nepoch: {epoch}, time: {T}, valid (NDCG@10: {t_valid[0]}, HR@10: {t_valid[1]})\"\n",
        "                )\n",
        "                print(\n",
        "                    f\"epoch: {epoch}, time: {T},  test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\"\n",
        "                )\n",
        "                t0.start()\n",
        "\n",
        "        t_test = self.evaluate(dataset)\n",
        "        print(f\"\\nepoch: {epoch}, test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\")\n",
        "\n",
        "        return t_test\n",
        "\n",
        "    def evaluate(self, dataset):\n",
        "        \"\"\"\n",
        "        Evaluation on the test users (users with at least 3 items)\n",
        "        \"\"\"\n",
        "        usernum = dataset.usernum\n",
        "        itemnum = dataset.itemnum\n",
        "        train = dataset.user_train  # removing deepcopy\n",
        "        valid = dataset.user_valid\n",
        "        test = dataset.user_test\n",
        "\n",
        "        NDCG = 0.0\n",
        "        HT = 0.0\n",
        "        valid_user = 0.0\n",
        "\n",
        "        if usernum > 1000:\n",
        "            users = random.sample(range(1, usernum + 1), 1000)\n",
        "        else:\n",
        "            users = range(1, usernum + 1)\n",
        "\n",
        "        for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "\n",
        "            if len(train[u]) < 1 or len(test[u]) < 1:\n",
        "                continue\n",
        "\n",
        "            seq = np.zeros([self.seq_max_len], dtype=np.int32)\n",
        "            idx = self.seq_max_len - 1\n",
        "            seq[idx] = valid[u][0]\n",
        "            idx -= 1\n",
        "            for i in reversed(train[u]):\n",
        "                seq[idx] = i\n",
        "                idx -= 1\n",
        "                if idx == -1:\n",
        "                    break\n",
        "            rated = set(train[u])\n",
        "            rated.add(0)\n",
        "            item_idx = [test[u][0]]\n",
        "            for _ in range(self.num_neg_test):\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "                while t in rated:\n",
        "                    t = np.random.randint(1, itemnum + 1)\n",
        "                item_idx.append(t)\n",
        "\n",
        "            inputs = {}\n",
        "            inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "            inputs[\"input_seq\"] = np.array([seq])\n",
        "            inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "            # inverse to get descending sort\n",
        "            predictions = -1.0 * self.predict(inputs)\n",
        "            predictions = np.array(predictions)\n",
        "            predictions = predictions[0]\n",
        "\n",
        "            rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "            valid_user += 1\n",
        "\n",
        "            if rank < 10:\n",
        "                NDCG += 1 / np.log2(rank + 2)\n",
        "                HT += 1\n",
        "\n",
        "        return NDCG / valid_user, HT / valid_user\n",
        "\n",
        "    def evaluate_valid(self, dataset):\n",
        "        \"\"\"\n",
        "        Evaluation on the validation users\n",
        "        \"\"\"\n",
        "        usernum = dataset.usernum\n",
        "        itemnum = dataset.itemnum\n",
        "        train = dataset.user_train  # removing deepcopy\n",
        "        valid = dataset.user_valid\n",
        "\n",
        "        NDCG = 0.0\n",
        "        valid_user = 0.0\n",
        "        HT = 0.0\n",
        "        if usernum > 1000:\n",
        "            users = random.sample(range(1, usernum + 1), 1000)\n",
        "        else:\n",
        "            users = range(1, usernum + 1)\n",
        "\n",
        "        for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "            if len(train[u]) < 1 or len(valid[u]) < 1:\n",
        "                continue\n",
        "\n",
        "            seq = np.zeros([self.seq_max_len], dtype=np.int32)\n",
        "            idx = self.seq_max_len - 1\n",
        "            for i in reversed(train[u]):\n",
        "                seq[idx] = i\n",
        "                idx -= 1\n",
        "                if idx == -1:\n",
        "                    break\n",
        "\n",
        "            rated = set(train[u])\n",
        "            rated.add(0)\n",
        "            item_idx = [valid[u][0]]\n",
        "            for _ in range(self.num_neg_test):\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "                while t in rated:\n",
        "                    t = np.random.randint(1, itemnum + 1)\n",
        "                item_idx.append(t)\n",
        "\n",
        "            inputs = {}\n",
        "            inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "            inputs[\"input_seq\"] = np.array([seq])\n",
        "            inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "            # predictions = -model.predict(sess, [u], [seq], item_idx)\n",
        "            predictions = -1.0 * self.predict(inputs)\n",
        "            predictions = np.array(predictions)\n",
        "            predictions = predictions[0]\n",
        "\n",
        "            rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "            valid_user += 1\n",
        "\n",
        "            if rank < 10:\n",
        "                NDCG += 1 / np.log2(rank + 2)\n",
        "                HT += 1\n",
        "\n",
        "        return NDCG / valid_user, HT / valid_user\n"
      ],
      "metadata": {
        "id": "C16F6-A0RTuu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "-TQyPInLPyoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "RCA9jMIJ0dIA",
        "outputId": "6dae31c0-0461-46bb-a77e-ab34420f9298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7458216, 8)\n",
            "(62818, 8)\n",
            "8398\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               timestamp            hit_id  \\\n",
              "145  2022-06-29 03:46:38  c568259112a24df7   \n",
              "184  2022-06-29 03:57:19  1489fd5444e14cc4   \n",
              "498  2022-06-29 01:39:10  5239ba594ab7481d   \n",
              "534  2022-06-29 23:29:41  198713b502454826   \n",
              "650  2022-06-30 04:28:09  f219c6a3fe7a45cc   \n",
              "\n",
              "                                      uid platform       event_name  \\\n",
              "145                              81855729  android  OpenOfferScreen   \n",
              "184                              92718060      ios  OpenOfferScreen   \n",
              "498                              85739900  android  OpenOfferScreen   \n",
              "534  a30530e4-3a2f-479b-b5a6-ffb58163e40f  android  OpenOfferScreen   \n",
              "650  bc9ffe7e-2cf7-472b-b509-212b03cab95a  android  OpenOfferScreen   \n",
              "\n",
              "                screen   offer_id    ptn_dadd  \n",
              "145    FavoritesScreen  274151298  2022-06-29  \n",
              "184  SearchResultsList  275160049  2022-06-29  \n",
              "498  SearchResultsList  269076943  2022-06-29  \n",
              "534          MapScreen  274706511  2022-06-30  \n",
              "650  SearchResultsList  267349310  2022-06-30  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3429a52d-4804-40f0-8e7a-8d1404ec7153\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>hit_id</th>\n",
              "      <th>uid</th>\n",
              "      <th>platform</th>\n",
              "      <th>event_name</th>\n",
              "      <th>screen</th>\n",
              "      <th>offer_id</th>\n",
              "      <th>ptn_dadd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>2022-06-29 03:46:38</td>\n",
              "      <td>c568259112a24df7</td>\n",
              "      <td>81855729</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>FavoritesScreen</td>\n",
              "      <td>274151298</td>\n",
              "      <td>2022-06-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>2022-06-29 03:57:19</td>\n",
              "      <td>1489fd5444e14cc4</td>\n",
              "      <td>92718060</td>\n",
              "      <td>ios</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>SearchResultsList</td>\n",
              "      <td>275160049</td>\n",
              "      <td>2022-06-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>2022-06-29 01:39:10</td>\n",
              "      <td>5239ba594ab7481d</td>\n",
              "      <td>85739900</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>SearchResultsList</td>\n",
              "      <td>269076943</td>\n",
              "      <td>2022-06-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>2022-06-29 23:29:41</td>\n",
              "      <td>198713b502454826</td>\n",
              "      <td>a30530e4-3a2f-479b-b5a6-ffb58163e40f</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>MapScreen</td>\n",
              "      <td>274706511</td>\n",
              "      <td>2022-06-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>2022-06-30 04:28:09</td>\n",
              "      <td>f219c6a3fe7a45cc</td>\n",
              "      <td>bc9ffe7e-2cf7-472b-b509-212b03cab95a</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>SearchResultsList</td>\n",
              "      <td>267349310</td>\n",
              "      <td>2022-06-30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3429a52d-4804-40f0-8e7a-8d1404ec7153')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3429a52d-4804-40f0-8e7a-8d1404ec7153 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3429a52d-4804-40f0-8e7a-8d1404ec7153');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# load the data\n",
        "data = pd.read_csv('./drive/MyDrive/ML/internship_clickstream_data.csv')\n",
        "embeddings = load_npz('./drive/MyDrive/ML/embeddings_test10000.npz').toarray()\n",
        "print(data.shape)\n",
        "\n",
        "# filter offer_id which are in embedding test file\n",
        "data.drop_duplicates(subset=['uid', 'offer_id'], inplace=True)\n",
        "ID = embeddings[:, 0]\n",
        "data = data[data['offer_id'].isin(ID)]\n",
        "print(data.shape)\n",
        "print(len(set(data['offer_id'])))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take users that have >5 clicks -- ПОКА НЕ ПОЛУЧАЕТСЯ:( НАДО СОХРАНИТЬ ФАЙЛ ПОБОЛЬШЕ\n",
        "#while not (data['offer_id'].value_counts()[data['offer_id'].value_counts() <= 5].empty) or \\\n",
        "#      not (data['uid'].value_counts()[data['uid'].value_counts() <= 5].empty):\n",
        "#    offer_ids = data['offer_id'].value_counts()[data['offer_id'].value_counts() > 5].index\n",
        "#    data = data[data['offer_id'].isin(offer_ids)]\n",
        "#    uids = data['uid'].value_counts()[data['uid'].value_counts() > 5].index\n",
        "#    data = data[data['uid'].isin(uids)]\n",
        "\n",
        "# encode, start with 1 - условие SASRec\n",
        "offer_encoder = {off: ind for ind, off in enumerate(data['offer_id'].unique())}\n",
        "data['offer_id_enc'] = data['offer_id'].map(offer_encoder) + 1\n",
        "uid_encoder = {uid: ind for ind, uid in enumerate(data['uid'].unique())}\n",
        "data['uid_enc'] = data['uid'].map(uid_encoder) + 1\n",
        "\n",
        "# sort by time and user id\n",
        "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
        "data.sort_values(by=['uid_enc', 'timestamp'], inplace=True)\n",
        "print(data.shape)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "yEcxl2STCsTz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "0c27c920-7522-45a0-fafc-20f61b15de65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(62818, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  timestamp            hit_id  \\\n",
              "145     2022-06-29 03:46:38  c568259112a24df7   \n",
              "6115536 2022-06-29 07:57:05  f3cde97ed57e4834   \n",
              "184     2022-06-29 03:57:19  1489fd5444e14cc4   \n",
              "498     2022-06-29 01:39:10  5239ba594ab7481d   \n",
              "3032143 2022-06-29 23:13:29  bebc324643034f2c   \n",
              "\n",
              "                                          uid platform       event_name  \\\n",
              "145                                  81855729  android  OpenOfferScreen   \n",
              "6115536                              81855729  android  OpenOfferScreen   \n",
              "184                                  92718060      ios  OpenOfferScreen   \n",
              "498                                  85739900  android  OpenOfferScreen   \n",
              "3032143  a30530e4-3a2f-479b-b5a6-ffb58163e40f  android  OpenOfferScreen   \n",
              "\n",
              "                    screen   offer_id    ptn_dadd  offer_id_enc  uid_enc  \n",
              "145        FavoritesScreen  274151298  2022-06-29             1        1  \n",
              "6115536    FavoritesScreen  274507200  2022-06-29          4503        1  \n",
              "184      SearchResultsList  275160049  2022-06-29             2        2  \n",
              "498      SearchResultsList  269076943  2022-06-29             3        3  \n",
              "3032143          MapScreen  273646044  2022-06-30           310        4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa6675d4-5ef2-46c2-ae9f-780c7cd19810\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>hit_id</th>\n",
              "      <th>uid</th>\n",
              "      <th>platform</th>\n",
              "      <th>event_name</th>\n",
              "      <th>screen</th>\n",
              "      <th>offer_id</th>\n",
              "      <th>ptn_dadd</th>\n",
              "      <th>offer_id_enc</th>\n",
              "      <th>uid_enc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>2022-06-29 03:46:38</td>\n",
              "      <td>c568259112a24df7</td>\n",
              "      <td>81855729</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>FavoritesScreen</td>\n",
              "      <td>274151298</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6115536</th>\n",
              "      <td>2022-06-29 07:57:05</td>\n",
              "      <td>f3cde97ed57e4834</td>\n",
              "      <td>81855729</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>FavoritesScreen</td>\n",
              "      <td>274507200</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>4503</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>2022-06-29 03:57:19</td>\n",
              "      <td>1489fd5444e14cc4</td>\n",
              "      <td>92718060</td>\n",
              "      <td>ios</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>SearchResultsList</td>\n",
              "      <td>275160049</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>2022-06-29 01:39:10</td>\n",
              "      <td>5239ba594ab7481d</td>\n",
              "      <td>85739900</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>SearchResultsList</td>\n",
              "      <td>269076943</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3032143</th>\n",
              "      <td>2022-06-29 23:13:29</td>\n",
              "      <td>bebc324643034f2c</td>\n",
              "      <td>a30530e4-3a2f-479b-b5a6-ffb58163e40f</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>MapScreen</td>\n",
              "      <td>273646044</td>\n",
              "      <td>2022-06-30</td>\n",
              "      <td>310</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa6675d4-5ef2-46c2-ae9f-780c7cd19810')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa6675d4-5ef2-46c2-ae9f-780c7cd19810 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa6675d4-5ef2-46c2-ae9f-780c7cd19810');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gkEtJ_SE37mF"
      },
      "outputs": [],
      "source": [
        "# create .txt file for input to model\n",
        "data[['offer_id_enc',\t'uid_enc']].to_csv('out.txt', sep=\"\\t\", header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x2rsJy_F8CAO"
      },
      "outputs": [],
      "source": [
        "# create special data format for SAS\n",
        "dataS = SASRecDataSet(filename='out.txt', col_sep='\\t')\n",
        "# split into train, test and validation\n",
        "dataS.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nX3Rq4Qs8CEa"
      },
      "outputs": [],
      "source": [
        "# model variables\n",
        "num_epochs = 5\n",
        "batch_size = 200\n",
        "RANDOM_SEED = 100  # Set None for non-deterministic result\n",
        "\n",
        "lr = 0.001             # learning rate\n",
        "maxlen = 50            # maximum sequence length for each user\n",
        "num_blocks = 2         # number of transformer blocks\n",
        "hidden_units = 100     # number of units in the attention calculation\n",
        "num_heads = 1          # number of attention heads\n",
        "dropout_rate = 0.1     # dropout rate\n",
        "l2_emb = 0.0           # L2 regularization coefficient\n",
        "num_neg_test = 8398    # number of negative examples per positive example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SGtPqTRP8CG_"
      },
      "outputs": [],
      "source": [
        "# sample negative examples\n",
        "sampler = WarpSampler(dataS.user_train, dataS.usernum, dataS.itemnum, batch_size=batch_size, maxlen=maxlen, n_workers=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qvJUdPEaNyMX"
      },
      "outputs": [],
      "source": [
        "model = SASREC(item_num=dataS.itemnum,\n",
        "               seq_max_len=maxlen,\n",
        "               num_blocks=num_blocks,\n",
        "               embedding_dim=hidden_units,\n",
        "               attention_dim=hidden_units,\n",
        "               attention_num_heads=num_heads,\n",
        "               dropout_rate=dropout_rate,\n",
        "               conv_dims = [100, 100],\n",
        "               l2_reg=l2_emb,\n",
        "               num_neg_test=num_neg_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYaJYll1N0Lv",
        "outputId": "95b40651-dba6-4e9a-a904-c217d9876c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 5, time: 12.829378885000096, valid (NDCG@10: 0.8441358024691358, HR@10: 0.8441358024691358)\n",
            "epoch: 5, time: 12.829378885000096,  test (NDCG@10: 0.9768211920529801, HR@10: 0.9768211920529801)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 5, test (NDCG@10: 0.9741518578352181, HR@10: 0.9741518578352181)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    t_test = model.train(dataS, sampler, num_epochs=num_epochs, batch_size=batch_size, lr=lr, val_epoch=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SASRec_with_embeddings.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}