{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaZharova/test_rec_systems/blob/main/SASRec_with_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_8zOYqp3D7yA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4181eedc-7150-490e-ff18-c0135149f65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting recommenders\n",
            "  Downloading recommenders-1.1.1-py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting scrapbook\n",
            "  Downloading scrapbook-0.5.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.11.2)\n",
            "Collecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.64.0)\n",
            "Collecting lightfm<2,>=1.15\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2<3.1,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.11.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.23.0)\n",
            "Collecting category-encoders<2,>=1.3.0\n",
            "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.2.2)\n",
            "Requirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.51.2)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.7.3)\n",
            "Collecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 51.5 MB/s \n",
            "\u001b[?25hCollecting transformers<5,>=2.5.0\n",
            "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 28.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.21.6)\n",
            "Collecting cornac<2,>=1.1.2\n",
            "  Downloading cornac-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 39.6 MB/s \n",
            "\u001b[?25hCollecting scikit-surprise>=1.0.6\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk<4,>=3.4 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.7)\n",
            "Collecting memory-profiler<1,>=0.54.0\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Collecting pandera[strategies]>=0.6.5\n",
            "  Downloading pandera-0.9.0-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.5)\n",
            "Collecting bottleneck<2,>=1.2.1\n",
            "  Downloading Bottleneck-1.3.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[K     |████████████████████████████████| 355 kB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.0.2)\n",
            "Requirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.2.3)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.2)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.10.2)\n",
            "Collecting powerlaw\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3.1,>=2->recommenders) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4,>=2.2.2->recommenders) (4.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.4.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders) (2022.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.9.1)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (21.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.14.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (6.0.1)\n",
            "Collecting hypothesis>=5.41.1\n",
            "  Downloading hypothesis-6.53.0-py3-none-any.whl (389 kB)\n",
            "\u001b[K     |████████████████████████████████| 389 kB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (2.4.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.0rc8-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (21.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (3.7.1)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 59.2 MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting papermill\n",
            "  Downloading papermill-2.3.4-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from scrapbook) (4.3.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from scrapbook) (5.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders) (3.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->scrapbook) (0.2.5)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (0.18.1)\n",
            "Collecting ansiwrap\n",
            "  Downloading ansiwrap-0.8.4-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (5.4.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (8.0.1)\n",
            "Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.6.6)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.4)\n",
            "Collecting traitlets>=4.2\n",
            "  Downloading traitlets-5.3.0-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 70.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (1.5.5)\n",
            "Collecting jupyter-client>=6.1.5\n",
            "  Downloading jupyter_client-7.3.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 73.1 MB/s \n",
            "\u001b[?25hCollecting tornado>=6.0\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[K     |████████████████████████████████| 423 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq>=23.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (23.2.0)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (4.11.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (2.16.1)\n",
            "Collecting textwrap3>=0.9.2\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->scrapbook) (0.7.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.2.1)\n",
            "Building wheels for collected packages: lightfm, memory-profiler, retrying, scikit-surprise\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705380 sha256=f93b1db9039a7ce6bb448e4f8c6f7b510c293d372259d26bc73b561725f6b580\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=c132654e6ccd98a41e484194b7c9f8a6050a658ab5b74f2fee43fac2044c6973\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=71ac89e92505be208dc81413269192636a502710f457e38191ee435e38d17f1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633726 sha256=ba1ffc5952287e550f5542c5e9d32485cada1b7ae052a13e6d91301417170031\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built lightfm memory-profiler retrying scikit-surprise\n",
            "Installing collected packages: traitlets, tornado, mypy-extensions, typing-inspect, textwrap3, pyyaml, jupyter-client, exceptiongroup, tokenizers, powerlaw, pandera, hypothesis, huggingface-hub, ansiwrap, transformers, scikit-surprise, retrying, papermill, memory-profiler, lightfm, cornac, category-encoders, bottleneck, scrapbook, recommenders\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.2 which is incompatible.\u001b[0m\n",
            "Successfully installed ansiwrap-0.8.4 bottleneck-1.3.5 category-encoders-1.3.0 cornac-1.14.2 exceptiongroup-1.0.0rc8 huggingface-hub-0.8.1 hypothesis-6.53.0 jupyter-client-7.3.4 lightfm-1.16 memory-profiler-0.60.0 mypy-extensions-0.4.3 pandera-0.9.0 papermill-2.3.4 powerlaw-1.5 pyyaml-5.4.1 recommenders-1.1.1 retrying-1.3.3 scikit-surprise-1.1.1 scrapbook-0.5.0 textwrap3-0.9.2 tokenizers-0.12.1 tornado-6.2 traitlets-5.3.0 transformers-4.21.0 typing-inspect-0.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install recommenders scrapbook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oEcypBYgz0Eu"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "from scipy.sparse import csr_matrix, load_npz\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.datasets.split_utils import filter_k_core\n",
        "\n",
        "# Transformer Based Models\n",
        "#from recommenders.models.sasrec.model import SASREC\n",
        "\n",
        "# Sampler for sequential prediction\n",
        "from recommenders.models.sasrec.sampler import WarpSampler\n",
        "from recommenders.models.sasrec.util import SASRecDataSet\n",
        "\n",
        "# Evaluation\n",
        "from recommenders.evaluation.python_evaluation import precision_at_k"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from recommenders.utils.timer import Timer\n",
        "#from recommenders.models.sasrec.model import MultiHeadAttention\n",
        "#from recommenders.models.sasrec.model import PointWiseFeedForward\n",
        "#from recommenders.models.sasrec.model import EncoderLayer\n",
        "from recommenders.models.sasrec.model import Encoder\n",
        "from recommenders.models.sasrec.model import LayerNormalization"
      ],
      "metadata": {
        "id": "TyD8MKGVWy4w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltWaoZ3JTqX0",
        "outputId": "9127e874-cbc6-4d1a-de66-df6362a936e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Changed SASRec code"
      ],
      "metadata": {
        "id": "NBnLymgJRTji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SASREC(tf.keras.Model):\n",
        "    \"\"\"SAS Rec model\n",
        "    Self-Attentive Sequential Recommendation Using Transformer\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        Wang-Cheng Kang, Julian McAuley (2018), Self-Attentive Sequential\n",
        "        Recommendation. Proceedings of IEEE International Conference on\n",
        "        Data Mining (ICDM'18)\n",
        "\n",
        "        Original source code from nnkkmto/SASRec-tf2,\n",
        "        https://github.com/nnkkmto/SASRec-tf2\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"Model initialization.\n",
        "\n",
        "        Args:\n",
        "            item_num (int): Number of items in the dataset.\n",
        "            seq_max_len (int): Maximum number of items in user history.\n",
        "            num_blocks (int): Number of Transformer blocks to be used.\n",
        "            embedding_dim (int): Item embedding dimension.\n",
        "            attention_dim (int): Transformer attention dimension.\n",
        "            conv_dims (list): List of the dimensions of the Feedforward layer.\n",
        "            dropout_rate (float): Dropout rate.\n",
        "            l2_reg (float): Coefficient of the L2 regularization.\n",
        "            num_neg_test (int): Number of negative examples used in testing.\n",
        "        \"\"\"\n",
        "        super(SASREC, self).__init__()\n",
        "\n",
        "        self.item_num = kwargs.get(\"item_num\", None)\n",
        "        self.seq_max_len = kwargs.get(\"seq_max_len\", 100)\n",
        "        self.num_blocks = kwargs.get(\"num_blocks\", 2)\n",
        "        self.embedding_dim = kwargs.get(\"embedding_dim\", 100)#### и сделать emb_dim pos\n",
        "        self.attention_dim = kwargs.get(\"attention_dim\", 100)\n",
        "        self.attention_num_heads = kwargs.get(\"attention_num_heads\", 1)\n",
        "        self.conv_dims = kwargs.get(\"conv_dims\", [100, 100])\n",
        "        self.dropout_rate = kwargs.get(\"dropout_rate\", 0.5)\n",
        "        self.l2_reg = kwargs.get(\"l2_reg\", 0.0)\n",
        "        self.num_neg_test = kwargs.get(\"num_neg_test\", 100)\n",
        "        self.embeddings = kwargs.get(\"embeddings\", None)\n",
        "\n",
        "        #vocab = self.embedding[:, 0]\n",
        "        embd = self.embeddings\n",
        "\n",
        "        #vocab_size = vocab.shape[0]\n",
        "        embedding_dim = embd.shape[0]\n",
        "        embedding = np.asarray(embd)\n",
        "        #del vocab\n",
        "        del embd\n",
        "        embedding = np.vstack((np.zeros((embedding.shape[1])), embedding))\n",
        "        \n",
        "        self.item_embedding_layer = tf.keras.layers.Embedding(\n",
        "            embedding.shape[0],\n",
        "            embedding.shape[1],\n",
        "            name=\"item_embeddings\",\n",
        "            mask_zero=True,\n",
        "            embeddings_regularizer=tf.keras.regularizers.L2(self.l2_reg),\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(embedding.astype(np.float32)), trainable=False)\n",
        "            # индекс строки - кодированные id_offer - Закодировать \n",
        "            # добавить нулевую строку\n",
        "\n",
        "        self.item_embedding_dense_layer = tf.keras.layers.Dense(100, activation='relu')\n",
        "        #print(type(self.item_embedding_layer))\n",
        "        #print(self.item_embedding_layer.shape)\n",
        "\n",
        "        self.positional_embedding_layer = tf.keras.layers.Embedding(\n",
        "            self.seq_max_len,\n",
        "            self.embedding_dim,\n",
        "            name=\"positional_embeddings\",\n",
        "            mask_zero=False,\n",
        "            embeddings_regularizer=tf.keras.regularizers.L2(self.l2_reg),\n",
        "        )\n",
        "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "        self.encoder = Encoder(\n",
        "            self.num_blocks,\n",
        "            self.seq_max_len,\n",
        "            self.embedding_dim,\n",
        "            self.attention_dim,\n",
        "            self.attention_num_heads,\n",
        "            self.conv_dims,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "        self.mask_layer = tf.keras.layers.Masking(mask_value=0)\n",
        "        self.layer_normalization = LayerNormalization(\n",
        "            self.seq_max_len, self.embedding_dim, 1e-08\n",
        "        )\n",
        "\n",
        "    def embedding(self, input_seq):\n",
        "        \"\"\"Compute the sequence and positional embeddings.\n",
        "\n",
        "        Args:\n",
        "            input_seq (tf.Tensor): Input sequence\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor:\n",
        "            - Sequence embeddings.\n",
        "            - Positional embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "        seq_embeddings = self.item_embedding_dense_layer(self.item_embedding_layer(input_seq))\n",
        "        seq_embeddings = seq_embeddings * (self.embedding_dim ** 0.5)\n",
        "\n",
        "        # FIXME\n",
        "        positional_seq = tf.expand_dims(tf.range(tf.shape(input_seq)[1]), 0)\n",
        "        positional_seq = tf.tile(positional_seq, [tf.shape(input_seq)[0], 1])\n",
        "        positional_embeddings = self.positional_embedding_layer(positional_seq)\n",
        "\n",
        "        return seq_embeddings, positional_embeddings\n",
        "\n",
        "    def call(self, x, training):\n",
        "        \"\"\"Model forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor.\n",
        "            training (tf.Tensor): Training tensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor, tf.Tensor:\n",
        "            - Logits of the positive examples.\n",
        "            - Logits of the negative examples.\n",
        "            - Mask for nonzero targets\n",
        "        \"\"\"\n",
        "\n",
        "        input_seq = x[\"input_seq\"]\n",
        "        pos = x[\"positive\"]\n",
        "        neg = x[\"negative\"]\n",
        "\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(input_seq, 0), tf.float32), -1)\n",
        "        seq_embeddings, positional_embeddings = self.embedding(input_seq)\n",
        "        #print(tf.shape(seq_embeddings), tf.shape(positional_embeddings))\n",
        "\n",
        "        # add positional embeddings\n",
        "        seq_embeddings += positional_embeddings\n",
        "\n",
        "        #print('PPP1')\n",
        "\n",
        "        # dropout\n",
        "        seq_embeddings = self.dropout_layer(seq_embeddings)\n",
        "\n",
        "        # masking\n",
        "        seq_embeddings *= mask\n",
        "\n",
        "        # --- ATTENTION BLOCKS ---\n",
        "        seq_attention = seq_embeddings\n",
        "        seq_attention = self.encoder(seq_attention, training, mask)\n",
        "        seq_attention = self.layer_normalization(seq_attention)  # (b, s, d)\n",
        "\n",
        "        # --- PREDICTION LAYER ---\n",
        "        # user's sequence embedding\n",
        "        pos = self.mask_layer(pos)\n",
        "        neg = self.mask_layer(neg)\n",
        "\n",
        "        #print('PPP2')\n",
        "\n",
        "        pos = tf.reshape(pos, [tf.shape(input_seq)[0] * self.seq_max_len])\n",
        "        neg = tf.reshape(neg, [tf.shape(input_seq)[0] * self.seq_max_len])\n",
        "        pos_emb = self.item_embedding_dense_layer(self.item_embedding_layer(pos))\n",
        "        neg_emb = self.item_embedding_dense_layer(self.item_embedding_layer(neg))\n",
        "        seq_emb = tf.reshape(\n",
        "            seq_attention,\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len, self.embedding_dim],\n",
        "        )  # (b*s, d)\n",
        "\n",
        "\n",
        "        #print('PPP3')\n",
        "        pos_logits = tf.reduce_sum(pos_emb * seq_emb, -1)\n",
        "        neg_logits = tf.reduce_sum(neg_emb * seq_emb, -1)\n",
        "\n",
        "        #print('PPP4')\n",
        "\n",
        "        pos_logits = tf.expand_dims(pos_logits, axis=-1)  # (bs, 1)\n",
        "        # pos_prob = tf.keras.layers.Dense(1, activation='sigmoid')(pos_logits)  # (bs, 1)\n",
        "        # 100, relu\n",
        "\n",
        "        neg_logits = tf.expand_dims(neg_logits, axis=-1)  # (bs, 1)\n",
        "        # neg_prob = tf.keras.layers.Dense(1, activation='sigmoid')(neg_logits)  # (bs, 1)\n",
        "\n",
        "        # output = tf.concat([pos_logits, neg_logits], axis=0)\n",
        "\n",
        "        # masking for loss calculation\n",
        "        istarget = tf.reshape(\n",
        "            tf.cast(tf.not_equal(pos, 0), dtype=tf.float32),\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len],\n",
        "        )\n",
        "\n",
        "        return pos_logits, neg_logits, istarget\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"Returns the logits for the test items.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "             tf.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        training = False\n",
        "        input_seq = inputs[\"input_seq\"]\n",
        "        candidate = inputs[\"candidate\"]\n",
        "\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(input_seq, 0), tf.float32), -1)\n",
        "        seq_embeddings, positional_embeddings = self.embedding(input_seq)\n",
        "        seq_embeddings += positional_embeddings\n",
        "        # seq_embeddings = self.dropout_layer(seq_embeddings)\n",
        "        seq_embeddings *= mask\n",
        "        seq_attention = seq_embeddings\n",
        "        seq_attention = self.encoder(seq_attention, training, mask)\n",
        "        seq_attention = self.layer_normalization(seq_attention)  # (b, s, d)\n",
        "        seq_emb = tf.reshape(\n",
        "            seq_attention,\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len, self.embedding_dim],\n",
        "        )  # (b*s, d)\n",
        "        candidate_emb = self.item_embedding_dense_layer(self.item_embedding_layer(candidate))  # (b, s, d)\n",
        "        candidate_emb = tf.transpose(candidate_emb, perm=[0, 2, 1])  # (b, d, s)\n",
        "        \n",
        "        #print('IN EVALUATE:', tf.shape(seq_emb), tf.shape(candidate_emb))\n",
        "        #print('\\n')\n",
        "        test_logits = tf.matmul(seq_emb, candidate_emb)\n",
        "        # (200, 100) * (1, 101, 100)'\n",
        "\n",
        "        test_logits = tf.reshape(\n",
        "            test_logits,\n",
        "            [tf.shape(input_seq)[0], self.seq_max_len, 1 + self.num_neg_test],\n",
        "        )  # (1, 200, 101)\n",
        "        test_logits = test_logits[:, -1, :]  # (1, 101)\n",
        "        return test_logits\n",
        "\n",
        "    def loss_function(self, pos_logits, neg_logits, istarget):\n",
        "        \"\"\"Losses are calculated separately for the positive and negative\n",
        "        items based on the corresponding logits. A mask is included to\n",
        "        take care of the zero items (added for padding).\n",
        "\n",
        "        Args:\n",
        "            pos_logits (tf.Tensor): Logits of the positive examples.\n",
        "            neg_logits (tf.Tensor): Logits of the negative examples.\n",
        "            istarget (tf.Tensor): Mask for nonzero targets.\n",
        "\n",
        "        Returns:\n",
        "            float: Loss.\n",
        "        \"\"\"\n",
        "\n",
        "        pos_logits = pos_logits[:, 0]\n",
        "        neg_logits = neg_logits[:, 0]\n",
        "\n",
        "        # ignore padding items (0)\n",
        "        # istarget = tf.reshape(\n",
        "        #     tf.cast(tf.not_equal(self.pos, 0), dtype=tf.float32),\n",
        "        #     [tf.shape(self.input_seq)[0] * self.seq_max_len],\n",
        "        # )\n",
        "        # for logits\n",
        "        loss = tf.reduce_sum(\n",
        "            -tf.math.log(tf.math.sigmoid(pos_logits) + 1e-24) * istarget\n",
        "            - tf.math.log(1 - tf.math.sigmoid(neg_logits) + 1e-24) * istarget\n",
        "        ) / tf.reduce_sum(istarget)\n",
        "\n",
        "        # for probabilities\n",
        "        # loss = tf.reduce_sum(\n",
        "        #         - tf.math.log(pos_logits + 1e-24) * istarget -\n",
        "        #         tf.math.log(1 - neg_logits + 1e-24) * istarget\n",
        "        # ) / tf.reduce_sum(istarget)\n",
        "        reg_loss = tf.compat.v1.losses.get_regularization_loss()\n",
        "        # reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n",
        "        # loss += sum(reg_losses)\n",
        "        loss += reg_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def create_combined_dataset(self, u, seq, pos, neg):\n",
        "        \"\"\"\n",
        "        function to create model inputs from sampled batch data.\n",
        "        This function is used only during training.\n",
        "        \"\"\"\n",
        "        inputs = {}\n",
        "        seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            seq, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "        pos = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            pos, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "        neg = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            neg, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "\n",
        "        inputs[\"users\"] = np.expand_dims(np.array(u), axis=-1)\n",
        "        inputs[\"input_seq\"] = seq\n",
        "        inputs[\"positive\"] = pos\n",
        "        inputs[\"negative\"] = neg\n",
        "\n",
        "        target = np.concatenate(\n",
        "            [\n",
        "                np.repeat(1, seq.shape[0] * seq.shape[1]),\n",
        "                np.repeat(0, seq.shape[0] * seq.shape[1]),\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        target = np.expand_dims(target, axis=-1)\n",
        "        return inputs, target\n",
        "\n",
        "    def train(self, dataset, sampler, **kwargs):\n",
        "        \"\"\"\n",
        "        High level function for model training as well as\n",
        "        evaluation on the validation and test dataset\n",
        "        \"\"\"\n",
        "\n",
        "        #print(\"IN TRAIN!\")\n",
        "\n",
        "        num_epochs = kwargs.get(\"num_epochs\", 10)\n",
        "        batch_size = kwargs.get(\"batch_size\", 128)\n",
        "        lr = kwargs.get(\"learning_rate\", 0.001)\n",
        "        val_epoch = kwargs.get(\"val_epoch\", 5)\n",
        "\n",
        "        num_steps = int(len(dataset.user_train) / batch_size)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-7\n",
        "        )\n",
        "\n",
        "        loss_function = self.loss_function\n",
        "\n",
        "        train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "        train_step_signature = [\n",
        "            {\n",
        "                \"users\": tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "                \"input_seq\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "                \"positive\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "                \"negative\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "            },\n",
        "            tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "        ]\n",
        "\n",
        "        @tf.function(input_signature=train_step_signature)\n",
        "        def train_step(inp, tar):\n",
        "            with tf.GradientTape() as tape:\n",
        "                pos_logits, neg_logits, loss_mask = self(inp, training=True)\n",
        "                loss = loss_function(pos_logits, neg_logits, loss_mask)\n",
        "\n",
        "            gradients = tape.gradient(loss, self.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "            train_loss(loss)\n",
        "            return loss\n",
        "\n",
        "        T = 0.0\n",
        "        t0 = Timer()\n",
        "        t0.start()\n",
        "\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "            step_loss = []\n",
        "            train_loss.reset_states()\n",
        "            for step in tqdm(\n",
        "                range(num_steps), total=num_steps, ncols=70, leave=False, unit=\"b\"\n",
        "            ):\n",
        "\n",
        "                u, seq, pos, neg = sampler.next_batch()\n",
        "\n",
        "                inputs, target = self.create_combined_dataset(u, seq, pos, neg)\n",
        "\n",
        "                loss = train_step(inputs, target)\n",
        "                step_loss.append(loss)\n",
        "\n",
        "            if epoch % val_epoch == 0:\n",
        "                t0.stop()\n",
        "                t1 = t0.interval\n",
        "                T += t1\n",
        "                print(\"Evaluating...\")\n",
        "                t_test = self.evaluate(dataset)\n",
        "                t_valid = self.evaluate_valid(dataset)\n",
        "                print(\n",
        "                    f\"\\nepoch: {epoch}, time: {T}, valid (NDCG@10: {t_valid[0]}, HR@10: {t_valid[1]})\"\n",
        "                )\n",
        "                print(\n",
        "                    f\"epoch: {epoch}, time: {T},  test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\"\n",
        "                )\n",
        "                t0.start()\n",
        "\n",
        "        t_test = self.evaluate(dataset)\n",
        "        print(f\"\\nepoch: {epoch}, test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\")\n",
        "\n",
        "        return t_test\n",
        "\n",
        "    def evaluate(self, dataset):\n",
        "        \"\"\"\n",
        "        Evaluation on the test users (users with at least 3 items)\n",
        "        \"\"\"\n",
        "        usernum = dataset.usernum\n",
        "        itemnum = dataset.itemnum\n",
        "        train = dataset.user_train  # removing deepcopy\n",
        "        valid = dataset.user_valid\n",
        "        test = dataset.user_test\n",
        "\n",
        "        NDCG = 0.0\n",
        "        HT = 0.0\n",
        "        valid_user = 0.0\n",
        "\n",
        "        if usernum > 1000:\n",
        "            users = random.sample(range(1, usernum + 1), 1000)\n",
        "        else:\n",
        "            users = range(1, usernum + 1)\n",
        "\n",
        "        for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "\n",
        "            if len(train[u]) < 1 or len(test[u]) < 1:\n",
        "                continue\n",
        "\n",
        "            seq = np.zeros([self.seq_max_len], dtype=np.int32)\n",
        "            idx = self.seq_max_len - 1\n",
        "            seq[idx] = valid[u][0]\n",
        "            idx -= 1\n",
        "            for i in reversed(train[u]):\n",
        "                seq[idx] = i\n",
        "                idx -= 1\n",
        "                if idx == -1:\n",
        "                    break\n",
        "            rated = set(train[u])\n",
        "            rated.add(0)\n",
        "            item_idx = [test[u][0]]\n",
        "            for _ in range(self.num_neg_test):\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "                while t in rated:\n",
        "                    t = np.random.randint(1, itemnum + 1)\n",
        "                item_idx.append(t)\n",
        "\n",
        "            inputs = {}\n",
        "            inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "            inputs[\"input_seq\"] = np.array([seq])\n",
        "            inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "            # inverse to get descending sort\n",
        "            predictions = -1.0 * self.predict(inputs)\n",
        "            predictions = np.array(predictions)\n",
        "            predictions = predictions[0]\n",
        "\n",
        "            rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "            valid_user += 1\n",
        "\n",
        "            if rank < 10:\n",
        "                NDCG += 1 / np.log2(rank + 2)\n",
        "                HT += 1\n",
        "\n",
        "        return NDCG / valid_user, HT / valid_user\n",
        "\n",
        "    def evaluate_valid(self, dataset):\n",
        "        \"\"\"\n",
        "        Evaluation on the validation users\n",
        "        \"\"\"\n",
        "        usernum = dataset.usernum\n",
        "        itemnum = dataset.itemnum\n",
        "        train = dataset.user_train  # removing deepcopy\n",
        "        valid = dataset.user_valid\n",
        "\n",
        "        NDCG = 0.0\n",
        "        valid_user = 0.0\n",
        "        HT = 0.0\n",
        "        if usernum > 1000:\n",
        "            users = random.sample(range(1, usernum + 1), 1000)\n",
        "        else:\n",
        "            users = range(1, usernum + 1)\n",
        "\n",
        "        for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "            if len(train[u]) < 1 or len(valid[u]) < 1:\n",
        "                continue\n",
        "\n",
        "            seq = np.zeros([self.seq_max_len], dtype=np.int32)\n",
        "            idx = self.seq_max_len - 1\n",
        "            for i in reversed(train[u]):\n",
        "                seq[idx] = i\n",
        "                idx -= 1\n",
        "                if idx == -1:\n",
        "                    break\n",
        "\n",
        "            rated = set(train[u])\n",
        "            rated.add(0)\n",
        "            item_idx = [valid[u][0]]\n",
        "            for _ in range(self.num_neg_test):\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "                while t in rated:\n",
        "                    t = np.random.randint(1, itemnum + 1)\n",
        "                item_idx.append(t)\n",
        "\n",
        "            inputs = {}\n",
        "            inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "            inputs[\"input_seq\"] = np.array([seq])\n",
        "            inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "            # predictions = -model.predict(sess, [u], [seq], item_idx)\n",
        "            predictions = -1.0 * self.predict(inputs)\n",
        "            predictions = np.array(predictions)\n",
        "            predictions = predictions[0]\n",
        "\n",
        "            rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "            valid_user += 1\n",
        "\n",
        "            if rank < 10:\n",
        "                NDCG += 1 / np.log2(rank + 2)\n",
        "                HT += 1\n",
        "\n",
        "        return NDCG / valid_user, HT / valid_user\n"
      ],
      "metadata": {
        "id": "C16F6-A0RTuu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "-TQyPInLPyoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "RCA9jMIJ0dIA",
        "outputId": "bc94ae02-4c3b-4877-91a5-70965f338f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7458216, 8)\n",
            "(4819809, 8)\n",
            "843725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             timestamp            hit_id  \\\n",
              "0  2022-06-29 01:04:03  4b45e714d01842a7   \n",
              "1  2022-06-29 01:06:10  e688e3349b35430f   \n",
              "2  2022-06-29 01:08:48  97c52e7a2e574f44   \n",
              "3  2022-06-29 01:09:39  d52e99bc7f7f4db8   \n",
              "4  2022-06-29 01:12:50  d49bf3352f64401c   \n",
              "\n",
              "                                    uid platform       event_name  \\\n",
              "0                              16650505      ios  OpenOfferScreen   \n",
              "1                              92346837  android  OpenOfferScreen   \n",
              "2  0bf81f10-ee3a-4543-a9ee-2bd12b4e4ec6  android  OpenOfferScreen   \n",
              "3                              84081279      ios  OpenOfferScreen   \n",
              "4  0bf81f10-ee3a-4543-a9ee-2bd12b4e4ec6  android  OpenOfferScreen   \n",
              "\n",
              "              screen   offer_id    ptn_dadd  \n",
              "0  SearchResultsList  274266785  2022-06-29  \n",
              "1          MapScreen  270671363  2022-06-29  \n",
              "2          Undefined  272968099  2022-06-29  \n",
              "3  SearchResultsList  268313499  2022-06-29  \n",
              "4          Undefined  255933042  2022-06-29  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d48da998-883f-403c-86a4-989cffebc61d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>hit_id</th>\n",
              "      <th>uid</th>\n",
              "      <th>platform</th>\n",
              "      <th>event_name</th>\n",
              "      <th>screen</th>\n",
              "      <th>offer_id</th>\n",
              "      <th>ptn_dadd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-06-29 01:04:03</td>\n",
              "      <td>4b45e714d01842a7</td>\n",
              "      <td>16650505</td>\n",
              "      <td>ios</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>SearchResultsList</td>\n",
              "      <td>274266785</td>\n",
              "      <td>2022-06-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-06-29 01:06:10</td>\n",
              "      <td>e688e3349b35430f</td>\n",
              "      <td>92346837</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>MapScreen</td>\n",
              "      <td>270671363</td>\n",
              "      <td>2022-06-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-06-29 01:08:48</td>\n",
              "      <td>97c52e7a2e574f44</td>\n",
              "      <td>0bf81f10-ee3a-4543-a9ee-2bd12b4e4ec6</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>272968099</td>\n",
              "      <td>2022-06-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-06-29 01:09:39</td>\n",
              "      <td>d52e99bc7f7f4db8</td>\n",
              "      <td>84081279</td>\n",
              "      <td>ios</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>SearchResultsList</td>\n",
              "      <td>268313499</td>\n",
              "      <td>2022-06-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-06-29 01:12:50</td>\n",
              "      <td>d49bf3352f64401c</td>\n",
              "      <td>0bf81f10-ee3a-4543-a9ee-2bd12b4e4ec6</td>\n",
              "      <td>android</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>255933042</td>\n",
              "      <td>2022-06-29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d48da998-883f-403c-86a4-989cffebc61d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d48da998-883f-403c-86a4-989cffebc61d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d48da998-883f-403c-86a4-989cffebc61d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# load the data\n",
        "data = pd.read_csv('./drive/MyDrive/internship_clickstream_data.csv')\n",
        "embeddings = np.load('./drive/MyDrive/embeddings_ALL.npy')\n",
        "print(data.shape)\n",
        "\n",
        "# remove duplicates and filter offer_id which are in embedding file\n",
        "data.drop_duplicates(subset=['uid', 'offer_id'], inplace=True)\n",
        "ID = embeddings[:, 0]\n",
        "data = data[data['offer_id'].isin(ID)]\n",
        "print(data.shape)\n",
        "print(len(set(data['offer_id'])))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take users that have > 5 clicks\n",
        "while not (data['offer_id'].value_counts()[data['offer_id'].value_counts() <= 5].empty) or \\\n",
        "      not (data['uid'].value_counts()[data['uid'].value_counts() <= 5].empty):\n",
        "    offer_ids = data['offer_id'].value_counts()[data['offer_id'].value_counts() > 5].index\n",
        "    data = data[data['offer_id'].isin(offer_ids)]\n",
        "    uids = data['uid'].value_counts()[data['uid'].value_counts() > 5].index\n",
        "    data = data[data['uid'].isin(uids)]\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvxrdhYiYO31",
        "outputId": "665dffda-e474-4da2-b9a6-3d91bddbfd30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2674879, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# оказывается, в файле с эмбеддингами есть дубликаты...\n",
        "print(embeddings[:, 0].shape)\n",
        "print(len(set(embeddings[:, 0])))\n",
        "unique_keys, indices = np.unique(embeddings[:, 0], return_index=True)\n",
        "embeddings = embeddings[np.sort(indices)]\n",
        "print(embeddings[:, 0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9LGuvkLkHh6",
        "outputId": "a5e16dee-6dce-4f38-93f2-8ccb098dc67d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1029010,)\n",
            "1029010\n",
            "(1029010,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_embed(embeds, offers):\n",
        "    \"\"\"\n",
        "        Filter embedding file, leaves only those that are in the dataset\n",
        "        INPUT: embeds, offers - numpy arrays (of lists)\n",
        "        OUTPUT: modified dataset\n",
        "    \"\"\"\n",
        "    emb_id = embeds[:, 0].astype(int)\n",
        "    mask = np.isin(emb_id, offers).reshape((embeds.shape[0], 1))\n",
        "    embeds = np.hstack((embeds, mask))\n",
        "    embeds = embeds[embeds[:, -1] != 0]\n",
        "    return embeds[:, :-1]\n",
        "offers = list(set(data['offer_id']))\n",
        "embeddings = filter_embed(embeddings, offers)\n",
        "print(len(offers), embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYL8ARKaRMrL",
        "outputId": "01381eac-b3ad-4d21-8f0d-e88a94ddee1c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156557 (156557, 185)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "offer_encoder = dict(zip(embeddings[:, 0], list(range(1, embeddings.shape[0] + 1))))\n",
        "data['offer_id_enc'] = data['offer_id'].map(offer_encoder)\n",
        "\n",
        "uid_encoder = {uid: ind for ind, uid in enumerate(data['uid'].unique())}\n",
        "data['uid_enc'] = data['uid'].map(uid_encoder) + 1\n",
        "\n",
        "# sort by time and user id\n",
        "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
        "data.sort_values(by=['uid_enc', 'timestamp'], inplace=True)\n",
        "print(data.shape)\n",
        "data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "x-FexV3R-R7Z",
        "outputId": "68e5ca36-882d-4f81-ae89-64b73175da03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2674879, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  timestamp            hit_id       uid platform  \\\n",
              "1542174 2022-06-28 04:44:41  2ddc8a7e74484809  16650505      ios   \n",
              "1542181 2022-06-28 04:46:54  a5ebdc71519949e4  16650505      ios   \n",
              "115025  2022-06-28 04:48:14  895cff66bfdc4b58  16650505      ios   \n",
              "5126428 2022-06-28 04:49:55  99eee7f8c31f4687  16650505      ios   \n",
              "6509    2022-06-28 04:53:23  0c2b184e62f04368  16650505      ios   \n",
              "\n",
              "              event_name     screen   offer_id    ptn_dadd  offer_id_enc  \\\n",
              "1542174  OpenOfferScreen  Undefined  275080365  2022-06-28         18052   \n",
              "1542181  OpenOfferScreen  MapScreen  271174333  2022-06-28         47138   \n",
              "115025   OpenOfferScreen  MapScreen  274706741  2022-06-28            26   \n",
              "5126428  OpenOfferScreen  MapScreen  261665602  2022-06-28         37702   \n",
              "6509     OpenOfferScreen  MapScreen  249093727  2022-06-28         30718   \n",
              "\n",
              "         uid_enc  \n",
              "1542174        1  \n",
              "1542181        1  \n",
              "115025         1  \n",
              "5126428        1  \n",
              "6509           1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c4d7f70-cfe3-4f14-9a0d-deea93a5dc51\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>hit_id</th>\n",
              "      <th>uid</th>\n",
              "      <th>platform</th>\n",
              "      <th>event_name</th>\n",
              "      <th>screen</th>\n",
              "      <th>offer_id</th>\n",
              "      <th>ptn_dadd</th>\n",
              "      <th>offer_id_enc</th>\n",
              "      <th>uid_enc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1542174</th>\n",
              "      <td>2022-06-28 04:44:41</td>\n",
              "      <td>2ddc8a7e74484809</td>\n",
              "      <td>16650505</td>\n",
              "      <td>ios</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>275080365</td>\n",
              "      <td>2022-06-28</td>\n",
              "      <td>18052</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542181</th>\n",
              "      <td>2022-06-28 04:46:54</td>\n",
              "      <td>a5ebdc71519949e4</td>\n",
              "      <td>16650505</td>\n",
              "      <td>ios</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>MapScreen</td>\n",
              "      <td>271174333</td>\n",
              "      <td>2022-06-28</td>\n",
              "      <td>47138</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115025</th>\n",
              "      <td>2022-06-28 04:48:14</td>\n",
              "      <td>895cff66bfdc4b58</td>\n",
              "      <td>16650505</td>\n",
              "      <td>ios</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>MapScreen</td>\n",
              "      <td>274706741</td>\n",
              "      <td>2022-06-28</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5126428</th>\n",
              "      <td>2022-06-28 04:49:55</td>\n",
              "      <td>99eee7f8c31f4687</td>\n",
              "      <td>16650505</td>\n",
              "      <td>ios</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>MapScreen</td>\n",
              "      <td>261665602</td>\n",
              "      <td>2022-06-28</td>\n",
              "      <td>37702</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6509</th>\n",
              "      <td>2022-06-28 04:53:23</td>\n",
              "      <td>0c2b184e62f04368</td>\n",
              "      <td>16650505</td>\n",
              "      <td>ios</td>\n",
              "      <td>OpenOfferScreen</td>\n",
              "      <td>MapScreen</td>\n",
              "      <td>249093727</td>\n",
              "      <td>2022-06-28</td>\n",
              "      <td>30718</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c4d7f70-cfe3-4f14-9a0d-deea93a5dc51')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c4d7f70-cfe3-4f14-9a0d-deea93a5dc51 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c4d7f70-cfe3-4f14-9a0d-deea93a5dc51');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(list(set(data['offer_id_enc'])))[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMEsbqArmL09",
        "outputId": "f1d9cd43-4302-4eca-d678-7a1d67476225"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gkEtJ_SE37mF"
      },
      "outputs": [],
      "source": [
        "# create .txt file for input to model\n",
        "data[['offer_id_enc',\t'uid_enc']].to_csv('out.txt', sep=\"\\t\", header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x2rsJy_F8CAO"
      },
      "outputs": [],
      "source": [
        "# create special data format for SAS\n",
        "dataS = SASRecDataSet(filename='out.txt', col_sep='\\t')\n",
        "# split into train, test and validation\n",
        "dataS.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nX3Rq4Qs8CEa"
      },
      "outputs": [],
      "source": [
        "# model variables\n",
        "num_epochs = 20\n",
        "batch_size = 200\n",
        "RANDOM_SEED = 100  # Set None for non-deterministic result\n",
        "\n",
        "lr = 0.001             # learning rate\n",
        "maxlen = 50            # maximum sequence length for each user\n",
        "num_blocks = 2         # number of transformer blocks\n",
        "hidden_units = 100     # number of units in the attention calculation\n",
        "num_heads = 1          # number of attention heads\n",
        "dropout_rate = 0.1     # dropout rate\n",
        "l2_emb = 0.0           # L2 regularization coefficient\n",
        "num_neg_test = len(set(data['offer_id_enc']))  # number of negative examples per positive example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SGtPqTRP8CG_"
      },
      "outputs": [],
      "source": [
        "# sample negative examples\n",
        "sampler = WarpSampler(dataS.user_train, dataS.usernum, dataS.itemnum, batch_size=batch_size, maxlen=maxlen, n_workers=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qvJUdPEaNyMX"
      },
      "outputs": [],
      "source": [
        "model = SASREC(item_num=dataS.itemnum,\n",
        "               seq_max_len=maxlen,\n",
        "               num_blocks=num_blocks,\n",
        "               embedding_dim=hidden_units,\n",
        "               attention_dim=hidden_units,\n",
        "               attention_num_heads=num_heads,\n",
        "               dropout_rate=dropout_rate,\n",
        "               conv_dims = [100, 100],\n",
        "               l2_reg=l2_emb,\n",
        "               num_neg_test=num_neg_test,\n",
        "               embeddings=embeddings[:, 1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYaJYll1N0Lv",
        "outputId": "0b98e78b-5c82-41a2-b28b-8d6d9a165830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|                                          | 0/782 [00:00<?, ?b/s]"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    t_test = model.train(dataS, sampler, num_epochs=num_epochs, batch_size=batch_size, lr=lr, val_epoch=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SASRec_with_embeddings.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}