{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SASRec_with_userEmb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaZharova/recsys_internship/blob/main/SASRec_with_userEmb_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "McrI6jHk-lFO",
        "outputId": "5bbbbd0f-f73b-4a09-94b4-568f98361960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting recommenders\n",
            "  Downloading recommenders-1.1.1-py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 32.2 MB/s \n",
            "\u001b[?25hCollecting scrapbook\n",
            "  Downloading scrapbook-0.5.0-py3-none-any.whl (34 kB)\n",
            "Collecting category-encoders<2,>=1.3.0\n",
            "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting memory-profiler<1,>=0.54.0\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.11.2)\n",
            "Requirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.2.2)\n",
            "Requirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.56.0)\n",
            "Collecting scikit-surprise>=1.0.6\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 54.0 MB/s \n",
            "\u001b[?25hCollecting bottleneck<2,>=1.2.1\n",
            "  Downloading Bottleneck-1.3.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[K     |████████████████████████████████| 355 kB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk<4,>=3.4 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.7)\n",
            "Collecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.23.0)\n",
            "Requirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.5)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.7.3)\n",
            "Collecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.64.0)\n",
            "Requirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.2.3)\n",
            "Collecting cornac<2,>=1.1.2\n",
            "  Downloading cornac-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 35.7 MB/s \n",
            "\u001b[?25hCollecting lightfm<2,>=1.15\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 61.1 MB/s \n",
            "\u001b[?25hCollecting transformers<5,>=2.5.0\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2<3.1,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.11.3)\n",
            "Collecting pandera[strategies]>=0.6.5\n",
            "  Downloading pandera-0.9.0-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.21.6)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.12.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.2)\n",
            "Collecting powerlaw\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3.1,>=2->recommenders) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4,>=2.2.2->recommenders) (4.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.4.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (2022.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (0.39.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (4.12.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders) (2022.2.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.14.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (6.0.1)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (21.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.9.2)\n",
            "Collecting hypothesis>=5.41.1\n",
            "  Downloading hypothesis-6.54.4-py3-none-any.whl (390 kB)\n",
            "\u001b[K     |████████████████████████████████| 390 kB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (2.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (22.1.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.0rc8-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.1.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.0 MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from scrapbook) (4.3.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from scrapbook) (7.9.0)\n",
            "Collecting papermill\n",
            "  Downloading papermill-2.4.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba<1,>=0.38.1->recommenders) (3.8.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (2.0.10)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->scrapbook) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->scrapbook) (0.2.5)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (0.18.1)\n",
            "Collecting nbclient>=0.2.0\n",
            "  Downloading nbclient-0.6.6-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 161 kB/s \n",
            "\u001b[?25hCollecting ansiwrap\n",
            "  Downloading ansiwrap-0.8.4-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (8.0.1)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (5.4.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.4)\n",
            "Requirement already satisfied: jupyter-client>=6.1.5 in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (6.1.12)\n",
            "Collecting traitlets>=4.2\n",
            "  Downloading traitlets-5.3.0-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (4.11.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (5.1.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (23.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (2.16.1)\n",
            "Collecting textwrap3>=0.9.2\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->scrapbook) (0.7.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.2.1)\n",
            "Building wheels for collected packages: lightfm, memory-profiler, retrying, scikit-surprise\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705369 sha256=b55a9e3369fcacdf36bb6b058afb30befc55fe8d3889e5748d076d250acd7239\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=0dd3801b5630c115858ef734545970176020017ad60acd67b5e50c920c4b9b19\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=7dc79030b0fd82161d267de53b0ef420908aaaa4e35021ea24896cf833dd88a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633964 sha256=5386459ea85e7d2b34a60dd4aa735aae7383e8e816fb69c2ba3586b95c27c324\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built lightfm memory-profiler retrying scikit-surprise\n",
            "Installing collected packages: traitlets, mypy-extensions, typing-inspect, textwrap3, pyyaml, nest-asyncio, exceptiongroup, tokenizers, powerlaw, pandera, nbclient, jedi, hypothesis, huggingface-hub, ansiwrap, transformers, scikit-surprise, retrying, papermill, memory-profiler, lightfm, cornac, category-encoders, bottleneck, scrapbook, recommenders\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "Successfully installed ansiwrap-0.8.4 bottleneck-1.3.5 category-encoders-1.3.0 cornac-1.14.2 exceptiongroup-1.0.0rc8 huggingface-hub-0.8.1 hypothesis-6.54.4 jedi-0.18.1 lightfm-1.16 memory-profiler-0.60.0 mypy-extensions-0.4.3 nbclient-0.6.6 nest-asyncio-1.5.5 pandera-0.9.0 papermill-2.4.0 powerlaw-1.5 pyyaml-5.4.1 recommenders-1.1.1 retrying-1.3.3 scikit-surprise-1.1.1 scrapbook-0.5.0 textwrap3-0.9.2 tokenizers-0.12.1 traitlets-5.3.0 transformers-4.21.1 typing-inspect-0.8.0\n"
          ]
        }
      ],
      "source": [
        "! pip install recommenders scrapbook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "from scipy.sparse import csr_matrix, load_npz\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.datasets.split_utils import filter_k_core\n",
        "\n",
        "# Transformer Based Models\n",
        "#from recommenders.models.sasrec.model import SASREC\n",
        "\n",
        "# Sampler for sequential prediction\n",
        "from recommenders.models.sasrec.sampler import WarpSampler\n",
        "from recommenders.models.sasrec.util import SASRecDataSet"
      ],
      "metadata": {
        "id": "4PGzubBnA7yF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from recommenders.models.sasrec.model import Encoder\n",
        "from recommenders.models.sasrec.model import LayerNormalization"
      ],
      "metadata": {
        "id": "webIAuvXBcU4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwJkJYahCCrK",
        "outputId": "ca53bab4-a5c4-4342-9fb9-6ce55586c1d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SASREC(tf.keras.Model):\n",
        "    \"\"\"SAS Rec model\n",
        "    Self-Attentive Sequential Recommendation Using Transformer\n",
        "    :Citation:\n",
        "        Wang-Cheng Kang, Julian McAuley (2018), Self-Attentive Sequential\n",
        "        Recommendation. Proceedings of IEEE International Conference on\n",
        "        Data Mining (ICDM'18)\n",
        "        Original source code from nnkkmto/SASRec-tf2,\n",
        "        https://github.com/nnkkmto/SASRec-tf2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"Model initialization.\n",
        "        Args:\n",
        "            item_num (int): Number of items in the dataset.\n",
        "            seq_max_len (int): Maximum number of items in user history.\n",
        "            num_blocks (int): Number of Transformer blocks to be used.\n",
        "            embedding_dim (int): Item embedding dimension.\n",
        "            attention_dim (int): Transformer attention dimension.\n",
        "            conv_dims (list): List of the dimensions of the Feedforward layer.\n",
        "            dropout_rate (float): Dropout rate.\n",
        "            l2_reg (float): Coefficient of the L2 regularization.\n",
        "            num_neg_test (int): Number of negative examples used in testing.\n",
        "        \"\"\"\n",
        "        super(SASREC, self).__init__()\n",
        "\n",
        "        self.item_num = kwargs.get(\"item_num\", None)\n",
        "        self.seq_max_len = kwargs.get(\"seq_max_len\", 100)\n",
        "        self.num_blocks = kwargs.get(\"num_blocks\", 2)\n",
        "        self.embedding_dim = kwargs.get(\"embedding_dim\", 100)\n",
        "        self.attention_dim = kwargs.get(\"attention_dim\", 100)\n",
        "        self.attention_num_heads = kwargs.get(\"attention_num_heads\", 1)\n",
        "        self.conv_dims = kwargs.get(\"conv_dims\", [100, 100])\n",
        "        self.dropout_rate = kwargs.get(\"dropout_rate\", 0.5)\n",
        "        self.l2_reg = kwargs.get(\"l2_reg\", 0.0)\n",
        "        self.num_neg_test = kwargs.get(\"num_neg_test\", 100)\n",
        "\n",
        "        self.item_embedding_layer = tf.keras.layers.Embedding(\n",
        "            self.item_num + 1,\n",
        "            self.embedding_dim,\n",
        "            name=\"item_embeddings\",\n",
        "            mask_zero=True,\n",
        "            embeddings_regularizer=tf.keras.regularizers.L2(self.l2_reg),\n",
        "        )\n",
        "\n",
        "        self.positional_embedding_layer = tf.keras.layers.Embedding(\n",
        "            self.seq_max_len,\n",
        "            self.embedding_dim,\n",
        "            name=\"positional_embeddings\",\n",
        "            mask_zero=False,\n",
        "            embeddings_regularizer=tf.keras.regularizers.L2(self.l2_reg),\n",
        "        )\n",
        "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "        self.encoder = Encoder(\n",
        "            self.num_blocks,\n",
        "            self.seq_max_len,\n",
        "            self.embedding_dim,\n",
        "            self.attention_dim,\n",
        "            self.attention_num_heads,\n",
        "            self.conv_dims,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "        self.mask_layer = tf.keras.layers.Masking(mask_value=0)\n",
        "        self.layer_normalization = LayerNormalization(\n",
        "            self.seq_max_len, self.embedding_dim, 1e-08\n",
        "        )\n",
        "\n",
        "    def embedding(self, input_seq):\n",
        "        \"\"\"Compute the sequence and positional embeddings.\n",
        "        Args:\n",
        "            input_seq (tf.Tensor): Input sequence\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor:\n",
        "            - Sequence embeddings.\n",
        "            - Positional embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "        seq_embeddings = self.item_embedding_layer(input_seq)\n",
        "        seq_embeddings = seq_embeddings * (self.embedding_dim ** 0.5)\n",
        "\n",
        "        # FIXME\n",
        "        positional_seq = tf.expand_dims(tf.range(tf.shape(input_seq)[1]), 0)\n",
        "        positional_seq = tf.tile(positional_seq, [tf.shape(input_seq)[0], 1])\n",
        "        positional_embeddings = self.positional_embedding_layer(positional_seq)\n",
        "\n",
        "        return seq_embeddings, positional_embeddings\n",
        "\n",
        "    def call(self, x, training):\n",
        "        \"\"\"Model forward pass.\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor.\n",
        "            training (tf.Tensor): Training tensor.\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor, tf.Tensor:\n",
        "            - Logits of the positive examples.\n",
        "            - Logits of the negative examples.\n",
        "            - Mask for nonzero targets\n",
        "        \"\"\"\n",
        "\n",
        "        input_seq = x[\"input_seq\"]\n",
        "        pos = x[\"positive\"]\n",
        "        neg = x[\"negative\"]\n",
        "\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(input_seq, 0), tf.float32), -1)\n",
        "        seq_embeddings, positional_embeddings = self.embedding(input_seq)\n",
        "\n",
        "        # add positional embeddings\n",
        "        seq_embeddings += positional_embeddings\n",
        "\n",
        "        # dropout\n",
        "        seq_embeddings = self.dropout_layer(seq_embeddings)\n",
        "\n",
        "        # masking\n",
        "        seq_embeddings *= mask\n",
        "\n",
        "        # --- ATTENTION BLOCKS ---\n",
        "        seq_attention = seq_embeddings\n",
        "        seq_attention = self.encoder(seq_attention, training, mask)\n",
        "        seq_attention = self.layer_normalization(seq_attention)  # (b, s, d)\n",
        "\n",
        "        # --- PREDICTION LAYER ---\n",
        "        # user's sequence embedding\n",
        "        pos = self.mask_layer(pos)\n",
        "        neg = self.mask_layer(neg)\n",
        "\n",
        "        pos = tf.reshape(pos, [tf.shape(input_seq)[0] * self.seq_max_len])\n",
        "        neg = tf.reshape(neg, [tf.shape(input_seq)[0] * self.seq_max_len])\n",
        "        pos_emb = self.item_embedding_layer(pos)\n",
        "        neg_emb = self.item_embedding_layer(neg)\n",
        "        seq_emb = tf.reshape(\n",
        "            seq_attention,\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len, self.embedding_dim],\n",
        "        )  # (b*s, d)\n",
        "\n",
        "        pos_logits = tf.reduce_sum(pos_emb * seq_emb, -1)\n",
        "        neg_logits = tf.reduce_sum(neg_emb * seq_emb, -1)\n",
        "\n",
        "        pos_logits = tf.expand_dims(pos_logits, axis=-1)  # (bs, 1)\n",
        "        # pos_prob = tf.keras.layers.Dense(1, activation='sigmoid')(pos_logits)  # (bs, 1)\n",
        "\n",
        "        neg_logits = tf.expand_dims(neg_logits, axis=-1)  # (bs, 1)\n",
        "        # neg_prob = tf.keras.layers.Dense(1, activation='sigmoid')(neg_logits)  # (bs, 1)\n",
        "\n",
        "        # output = tf.concat([pos_logits, neg_logits], axis=0)\n",
        "\n",
        "        # masking for loss calculation\n",
        "        istarget = tf.reshape(\n",
        "            tf.cast(tf.not_equal(pos, 0), dtype=tf.float32),\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len],\n",
        "        )\n",
        "\n",
        "        return pos_logits, neg_logits, istarget\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"Returns the logits for the test items.\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Input tensor.\n",
        "        Returns:\n",
        "             tf.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        training = False\n",
        "        input_seq = inputs[\"input_seq\"]\n",
        "        candidate = inputs[\"candidate\"]\n",
        "\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(input_seq, 0), tf.float32), -1)\n",
        "        seq_embeddings, positional_embeddings = self.embedding(input_seq)\n",
        "        seq_embeddings += positional_embeddings\n",
        "        # seq_embeddings = self.dropout_layer(seq_embeddings)\n",
        "        seq_embeddings *= mask\n",
        "        seq_attention = seq_embeddings\n",
        "        seq_attention = self.encoder(seq_attention, training, mask)\n",
        "        seq_attention = self.layer_normalization(seq_attention)  # (b, s, d)\n",
        "        seq_emb = tf.reshape(\n",
        "            seq_attention,\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len, self.embedding_dim],\n",
        "        )  # (b*s, d)\n",
        "        candidate_emb = self.item_embedding_layer(candidate)  # (b, s, d)\n",
        "        candidate_emb = tf.transpose(candidate_emb, perm=[0, 2, 1])  # (b, d, s)\n",
        "\n",
        "        test_logits = tf.matmul(seq_emb, candidate_emb)\n",
        "        # (200, 100) * (1, 101, 100)'\n",
        "\n",
        "        test_logits = tf.reshape(\n",
        "            test_logits,\n",
        "            [tf.shape(input_seq)[0], self.seq_max_len, 1 + self.num_neg_test],\n",
        "        )  # (1, 200, 101)\n",
        "        test_logits = test_logits[:, -1, :]  # (1, 101)\n",
        "        return test_logits\n",
        "\n",
        "    def loss_function(self, pos_logits, neg_logits, istarget):\n",
        "        \"\"\"Losses are calculated separately for the positive and negative\n",
        "        items based on the corresponding logits. A mask is included to\n",
        "        take care of the zero items (added for padding).\n",
        "        Args:\n",
        "            pos_logits (tf.Tensor): Logits of the positive examples.\n",
        "            neg_logits (tf.Tensor): Logits of the negative examples.\n",
        "            istarget (tf.Tensor): Mask for nonzero targets.\n",
        "        Returns:\n",
        "            float: Loss.\n",
        "        \"\"\"\n",
        "\n",
        "        pos_logits = pos_logits[:, 0]\n",
        "        neg_logits = neg_logits[:, 0]\n",
        "\n",
        "        # ignore padding items (0)\n",
        "        # istarget = tf.reshape(\n",
        "        #     tf.cast(tf.not_equal(self.pos, 0), dtype=tf.float32),\n",
        "        #     [tf.shape(self.input_seq)[0] * self.seq_max_len],\n",
        "        # )\n",
        "        # for logits\n",
        "        loss = tf.reduce_sum(\n",
        "            -tf.math.log(tf.math.sigmoid(pos_logits) + 1e-24) * istarget\n",
        "            - tf.math.log(1 - tf.math.sigmoid(neg_logits) + 1e-24) * istarget\n",
        "        ) / tf.reduce_sum(istarget)\n",
        "\n",
        "        # for probabilities\n",
        "        # loss = tf.reduce_sum(\n",
        "        #         - tf.math.log(pos_logits + 1e-24) * istarget -\n",
        "        #         tf.math.log(1 - neg_logits + 1e-24) * istarget\n",
        "        # ) / tf.reduce_sum(istarget)\n",
        "        reg_loss = tf.compat.v1.losses.get_regularization_loss()\n",
        "        # reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n",
        "        # loss += sum(reg_losses)\n",
        "        loss += reg_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def create_combined_dataset(self, u, seq, pos, neg):\n",
        "        \"\"\"\n",
        "        function to create model inputs from sampled batch data.\n",
        "        This function is used only during training.\n",
        "        \"\"\"\n",
        "        inputs = {}\n",
        "        seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            seq, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "        pos = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            pos, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "        neg = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            neg, padding=\"pre\", truncating=\"pre\", maxlen=self.seq_max_len\n",
        "        )\n",
        "\n",
        "        inputs[\"users\"] = np.expand_dims(np.array(u), axis=-1)\n",
        "        inputs[\"input_seq\"] = seq\n",
        "        inputs[\"positive\"] = pos\n",
        "        inputs[\"negative\"] = neg\n",
        "\n",
        "        target = np.concatenate(\n",
        "            [\n",
        "                np.repeat(1, seq.shape[0] * seq.shape[1]),\n",
        "                np.repeat(0, seq.shape[0] * seq.shape[1]),\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        target = np.expand_dims(target, axis=-1)\n",
        "        return inputs, target\n",
        "\n",
        "    def get_user_embedding(self, inputs):\n",
        "        \"\"\"Returns embedding for ine user (by input seq).\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "             np.ndarray\n",
        "        \"\"\"\n",
        "        training = False\n",
        "        input_seq = inputs[\"input_seq\"]\n",
        "        candidate = inputs[\"candidate\"]\n",
        "\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(input_seq, 0), tf.float32), -1)\n",
        "        seq_embeddings, positional_embeddings = self.embedding(input_seq)\n",
        "        seq_embeddings += positional_embeddings\n",
        "        # seq_embeddings = self.dropout_layer(seq_embeddings)\n",
        "        seq_embeddings *= mask\n",
        "        seq_attention = seq_embeddings\n",
        "        seq_attention = self.encoder(seq_attention, training, mask)\n",
        "        seq_attention = self.layer_normalization(seq_attention)  # (b, s, d)\n",
        "        seq_emb = tf.reshape(\n",
        "            seq_attention,\n",
        "            [tf.shape(input_seq)[0] * self.seq_max_len, self.embedding_dim],\n",
        "        )  # (b*s, d)\n",
        "\n",
        "        return seq_emb.numpy()[np.where(mask.numpy().reshape(-1, 1) == 1)[0]].mean(axis=0)\n",
        "\n",
        "    \"\"\"def train(self, dataset, sampler, **kwargs):\n",
        "        \n",
        "        #High level function for model training as well as\n",
        "        #evaluation on the validation and test dataset\n",
        "        \n",
        "        num_epochs = kwargs.get(\"num_epochs\", 10)\n",
        "        batch_size = kwargs.get(\"batch_size\", 128)\n",
        "        lr = kwargs.get(\"learning_rate\", 0.001)\n",
        "        val_epoch = kwargs.get(\"val_epoch\", 5)\n",
        "\n",
        "        num_steps = int(len(dataset.user_train) / batch_size)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-7\n",
        "        )\n",
        "\n",
        "        loss_function = self.loss_function\n",
        "\n",
        "        train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "        train_step_signature = [\n",
        "            {\n",
        "                \"users\": tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "                \"input_seq\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "                \"positive\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "                \"negative\": tf.TensorSpec(\n",
        "                    shape=(None, self.seq_max_len), dtype=tf.int64\n",
        "                ),\n",
        "            },\n",
        "            tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "        ]\n",
        "\n",
        "        @tf.function(input_signature=train_step_signature)\n",
        "        def train_step(inp, tar):\n",
        "            with tf.GradientTape() as tape:\n",
        "                pos_logits, neg_logits, loss_mask = self(inp, training=True)\n",
        "                loss = loss_function(pos_logits, neg_logits, loss_mask)\n",
        "\n",
        "            gradients = tape.gradient(loss, self.trainable_variables)\n",
        "            #gradients = [(tf.clip_by_value(grad, -1.0, 1.0)) for grad in gradients]\n",
        "            optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "            train_loss(loss)\n",
        "            return loss\n",
        "\n",
        "        T = 0.0\n",
        "        t0 = Timer()\n",
        "        t0.start()\n",
        "\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "            WATCH_LOSS = 0\n",
        "            counter = 0\n",
        "            step_loss = []\n",
        "            train_loss.reset_states()\n",
        "            for step in tqdm(\n",
        "                range(num_steps), total=num_steps, ncols=70, leave=False, unit=\"b\"\n",
        "            ):\n",
        "\n",
        "                u, seq, pos, neg = sampler.next_batch()\n",
        "\n",
        "                inputs, target = self.create_combined_dataset(u, seq, pos, neg)\n",
        "\n",
        "                loss = train_step(inputs, target)\n",
        "                step_loss.append(loss)\n",
        "                WATCH_LOSS += loss\n",
        "                counter += 1\n",
        "            \n",
        "            print(f\"\\nIn {epoch} epoch the LOSS is {WATCH_LOSS/counter}\")\n",
        "            # self.save\n",
        "\n",
        "            if epoch % val_epoch == 0:\n",
        "                t0.stop()\n",
        "                t1 = t0.interval\n",
        "                T += t1\n",
        "                print(\"Evaluating...\")\n",
        "                t_test = self.evaluate(dataset)\n",
        "                t_valid = self.evaluate_valid(dataset)\n",
        "                print(\n",
        "                    f\"\\nepoch: {epoch}, time: {T}, valid (NDCG@10: {t_valid[0]}, HR@10: {t_valid[1]})\"\n",
        "                )\n",
        "                print(\n",
        "                    f\"epoch: {epoch}, time: {T},  test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\"\n",
        "                )\n",
        "                t0.start()\n",
        "\n",
        "            if epoch == num_epochs:\n",
        "                t0.stop()\n",
        "                t1 = t0.interval\n",
        "                T += t1\n",
        "                print(\"Getting user's embeddings...\")\n",
        "                user_embeddings = self.get_all_user_embeddings(dataset)\n",
        "                print(\"Nice:)\")\n",
        "                t0.start()\n",
        "\n",
        "        t_test = self.evaluate(dataset)\n",
        "        print(f\"\\nepoch: {epoch}, test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\")\n",
        "\n",
        "        return t_test, user_embeddings\n",
        "\n",
        "    def get_all_user_embeddings(self, dataset):\n",
        "        \n",
        "        #dhgkhdkjghdhfg\n",
        "        \n",
        "        usernum = dataset.usernum\n",
        "        itemnum = dataset.itemnum\n",
        "        train = dataset.user_train  # removing deepcopy\n",
        "        valid = dataset.user_valid\n",
        "        test = dataset.user_test\n",
        "\n",
        "        # =========== усечённый\n",
        "        if usernum > 5000:\n",
        "            users = random.sample(range(1, usernum + 1), 5000)\n",
        "        else:\n",
        "            users = range(1, usernum + 1)\n",
        "        #users = range(1, usernum + 1)   ====== вариант для всех пользаков\n",
        "        \n",
        "\n",
        "        all_users_embeddings = []\n",
        "\n",
        "        for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "\n",
        "            if len(train[u]) < 1 or len(test[u]) < 1:\n",
        "                continue\n",
        "\n",
        "            seq = np.zeros([self.seq_max_len], dtype=np.int32)\n",
        "            idx = self.seq_max_len - 1\n",
        "            seq[idx] = valid[u][0]\n",
        "            idx -= 1\n",
        "            for i in reversed(train[u]):\n",
        "                seq[idx] = i\n",
        "                idx -= 1\n",
        "                if idx == -1:\n",
        "                    break\n",
        "            rated = set(train[u])\n",
        "            rated.add(0)\n",
        "            item_idx = [test[u][0]]\n",
        "            for _ in range(self.num_neg_test):\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "                while t in rated:\n",
        "                    t = np.random.randint(1, itemnum + 1)\n",
        "                item_idx.append(t)\n",
        "\n",
        "            inputs = {}\n",
        "            inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "            inputs[\"input_seq\"] = np.array([seq])\n",
        "            inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "            all_users_embeddings.append(self.get_user_embedding(inputs))\n",
        "\n",
        "        return all_users_embeddings\"\"\"\n",
        "\n",
        "    def evaluate(self, dataset):\n",
        "        \"\"\"\n",
        "        Evaluation on the test users (users with at least 3 items)\n",
        "        \"\"\"\n",
        "        usernum = dataset.usernum\n",
        "        itemnum = dataset.itemnum\n",
        "        train = dataset.user_train  # removing deepcopy\n",
        "        valid = dataset.user_valid\n",
        "        test = dataset.user_test\n",
        "\n",
        "        NDCG = 0.0\n",
        "        HT = 0.0\n",
        "        valid_user = 0.0\n",
        "\n",
        "        if usernum > 100:\n",
        "            users = random.sample(range(1, usernum + 1), 100)\n",
        "        else:\n",
        "            users = range(1, usernum + 1)\n",
        "\n",
        "        for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "\n",
        "            if len(train[u]) < 1 or len(test[u]) < 1:\n",
        "                continue\n",
        "\n",
        "            seq = np.zeros([self.seq_max_len], dtype=np.int32)\n",
        "            idx = self.seq_max_len - 1\n",
        "            seq[idx] = valid[u][0]\n",
        "            idx -= 1\n",
        "            for i in reversed(train[u]):\n",
        "                seq[idx] = i\n",
        "                idx -= 1\n",
        "                if idx == -1:\n",
        "                    break\n",
        "            rated = set(train[u])\n",
        "            rated.add(0)\n",
        "            item_idx = [test[u][0]]\n",
        "            for _ in range(self.num_neg_test):\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "                while t in rated:\n",
        "                    t = np.random.randint(1, itemnum + 1)\n",
        "                item_idx.append(t)\n",
        "\n",
        "            inputs = {}\n",
        "            inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "            inputs[\"input_seq\"] = np.array([seq])\n",
        "            inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "            # inverse to get descending sort\n",
        "            predictions = -1.0 * self.predict(inputs)\n",
        "            predictions = np.array(predictions)\n",
        "            predictions = predictions[0]\n",
        "\n",
        "            rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "            valid_user += 1\n",
        "\n",
        "            if rank < 10:\n",
        "                NDCG += 1 / np.log2(rank + 2)\n",
        "                HT += 1\n",
        "\n",
        "        return NDCG / valid_user, HT / valid_user\n",
        "\n",
        "    def evaluate_valid(self, dataset):\n",
        "        \"\"\"\n",
        "        Evaluation on the validation users\n",
        "        \"\"\"\n",
        "        usernum = dataset.usernum\n",
        "        itemnum = dataset.itemnum\n",
        "        train = dataset.user_train  # removing deepcopy\n",
        "        valid = dataset.user_valid\n",
        "\n",
        "        NDCG = 0.0\n",
        "        valid_user = 0.0\n",
        "        HT = 0.0\n",
        "        if usernum > 100:\n",
        "            users = random.sample(range(1, usernum + 1), 100)\n",
        "        else:\n",
        "            users = range(1, usernum + 1)\n",
        "\n",
        "        for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "            if len(train[u]) < 1 or len(valid[u]) < 1:\n",
        "                continue\n",
        "\n",
        "            seq = np.zeros([self.seq_max_len], dtype=np.int32)\n",
        "            idx = self.seq_max_len - 1\n",
        "            for i in reversed(train[u]):\n",
        "                seq[idx] = i\n",
        "                idx -= 1\n",
        "                if idx == -1:\n",
        "                    break\n",
        "\n",
        "            rated = set(train[u])\n",
        "            rated.add(0)\n",
        "            item_idx = [valid[u][0]]\n",
        "            for _ in range(self.num_neg_test):\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "                while t in rated:\n",
        "                    t = np.random.randint(1, itemnum + 1)\n",
        "                item_idx.append(t)\n",
        "\n",
        "            inputs = {}\n",
        "            inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "            inputs[\"input_seq\"] = np.array([seq])\n",
        "            inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "            # predictions = -model.predict(sess, [u], [seq], item_idx)\n",
        "            predictions = -1.0 * self.predict(inputs)\n",
        "            predictions = np.array(predictions)\n",
        "            predictions = predictions[0]\n",
        "\n",
        "            rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "            valid_user += 1\n",
        "\n",
        "            if rank < 10:\n",
        "                NDCG += 1 / np.log2(rank + 2)\n",
        "                HT += 1\n",
        "\n",
        "        return NDCG / valid_user, HT / valid_user"
      ],
      "metadata": {
        "id": "eL-StMVGmVBr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выделим пользователей для обучения и для построения разложений эмбедов\n",
        "\n",
        "Можно в дальнейшем не запускать эту часть кода"
      ],
      "metadata": {
        "id": "yl229yzvZgUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_txt = pd.read_csv('./drive/MyDrive/Data_ML/out_1808.txt', sep='\\t', header=None)\n",
        "out_txt.columns = ['user_id', 'offer_id']\n",
        "out_txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2bc4FRrGZAXO",
        "outputId": "07a6b977-ac68-44c2-b922-6bca3538f056"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         user_id  offer_id\n",
              "0              1         1\n",
              "1              1         2\n",
              "2              1         3\n",
              "3              1         4\n",
              "4              1         5\n",
              "...          ...       ...\n",
              "8663642   431982    192929\n",
              "8663643   431982    220742\n",
              "8663644   431982    277534\n",
              "8663645   431982    156059\n",
              "8663646   431982    220737\n",
              "\n",
              "[8663647 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf044403-1504-4b11-a298-27709c972eea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>offer_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8663642</th>\n",
              "      <td>431982</td>\n",
              "      <td>192929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8663643</th>\n",
              "      <td>431982</td>\n",
              "      <td>220742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8663644</th>\n",
              "      <td>431982</td>\n",
              "      <td>277534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8663645</th>\n",
              "      <td>431982</td>\n",
              "      <td>156059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8663646</th>\n",
              "      <td>431982</td>\n",
              "      <td>220737</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8663647 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf044403-1504-4b11-a298-27709c972eea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf044403-1504-4b11-a298-27709c972eea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf044403-1504-4b11-a298-27709c972eea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# выделяем ползователей для модели и для разложений\n",
        "#random.seed(10)\n",
        "#random_users = random.sample(range(1, out_txt['user_id'].max() + 1), 5000)\n",
        "model_out = out_txt[out_txt['user_id'] < 420000]\n",
        "pcatsne_out = out_txt[out_txt['user_id'] >= 420000]\n",
        "print(model_out.shape, pcatsne_out.shape)\n",
        "print(model_out['user_id'].unique().shape, pcatsne_out['user_id'].unique().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_t3j-Y8bQl1",
        "outputId": "d9231fb8-2a96-4aa2-c735-0776d455d6fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8558387, 2) (105260, 2)\n",
            "(419999,) (11983,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# отфильтруем так, чтобы в файле для разложений были только объявки из тех, что покажем модели\n",
        "print('Bef:', pcatsne_out.shape, pcatsne_out['user_id'].unique().shape, pcatsne_out['offer_id'].unique().shape)\n",
        "model_offers = model_out['offer_id'].unique()\n",
        "pcatsne_out = pcatsne_out[pcatsne_out['offer_id'].isin(model_offers)]\n",
        "print('Aft:', pcatsne_out.shape, pcatsne_out['user_id'].unique().shape, pcatsne_out['offer_id'].unique().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGxPI4K6ZAZ3",
        "outputId": "dc06734d-8230-4d99-86ec-584e817f2361"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bef: (105260, 2) (11983,) (81061,)\n",
            "Aft: (105260, 2) (11983,) (81061,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# сохраним файлы с данными\n",
        "model_out[['user_id', 'offer_id']].to_csv('./drive/MyDrive/out_model.txt', sep='\\t', header=False, index=False)\n",
        "pcatsne_out[['user_id', 'offer_id']].to_csv('./drive/MyDrive/out_pcatsne.txt', sep='\\t', header=False, index=False)"
      ],
      "metadata": {
        "id": "jkDPohydggzZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Переходим к модели"
      ],
      "metadata": {
        "id": "lG91QSnHhFl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create special data format for SAS\n",
        "data_model = SASRecDataSet(filename='./drive/MyDrive/out_model.txt', col_sep='\\t')\n",
        "# split into train, test and validation\n",
        "data_model.split()\n",
        "\n",
        "data_pcatsne = SASRecDataSet(filename='./drive/MyDrive/out_pcatsne.txt', col_sep='\\t')\n",
        "data_pcatsne.split()"
      ],
      "metadata": {
        "id": "64yQm7KGC3o_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_model.usernum)\n",
        "print(data_model.itemnum)\n",
        "print(data_pcatsne.usernum)\n",
        "print(data_pcatsne.itemnum)"
      ],
      "metadata": {
        "id": "d3K9kshQF3I4",
        "outputId": "217d88e0-8cff-4145-c51b-3158158bd9d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "419999\n",
            "388607\n",
            "431982\n",
            "388606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(list(data_pcatsne.User.keys()))"
      ],
      "metadata": {
        "id": "JhnGITfhv0UI",
        "outputId": "6dbf4620-2c08-46a9-e693-ee5a56b3a32a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "431982"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model variables\n",
        "num_epochs = 11\n",
        "batch_size = 200   #                                                         !!!\n",
        "RANDOM_SEED = 100  # Set None for non-deterministic result\n",
        "lr = 0.001             # learning rate                                       !!!\n",
        "maxlen = 50            # maximum sequence length for each user\n",
        "num_blocks = 2         # number of transformer blocks\n",
        "hidden_units = 100     # number of units in the attention calculation.       !!!\n",
        "num_heads = 1          # number of attention heads\n",
        "dropout_rate = 0.1     # dropout rate\n",
        "l2_emb = 0.0           # L2 regularization coefficient                       !!!\n",
        "num_neg_test = data_model.itemnum  # number of negative examples per positive example\n"
      ],
      "metadata": {
        "id": "4mtKS4QAC3z6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample negative examples\n",
        "sampler = WarpSampler(data_model.user_train, data_model.usernum, data_model.itemnum, batch_size=batch_size, maxlen=maxlen, n_workers=3)"
      ],
      "metadata": {
        "id": "CMhP_cM4C31z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SASREC(item_num=data_model.itemnum,\n",
        "               seq_max_len=maxlen,\n",
        "               num_blocks=num_blocks,\n",
        "               embedding_dim=hidden_units,\n",
        "               attention_dim=hidden_units,\n",
        "               attention_num_heads=num_heads,\n",
        "               dropout_rate=dropout_rate,\n",
        "               conv_dims = [hidden_units, hidden_units],         ### !!!\n",
        "               l2_reg=l2_emb,\n",
        "               num_neg_test=num_neg_test)"
      ],
      "metadata": {
        "id": "IOEQCae1C34U"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_user_embeddings(model, dataset):\n",
        "    \"\"\"\n",
        "        Returns embeddings for fixed users\n",
        "    \"\"\"\n",
        "    usernum = dataset.usernum\n",
        "    itemnum = dataset.itemnum\n",
        "    train = dataset.user_train  # removing deepcopy\n",
        "    valid = dataset.user_valid\n",
        "    test = dataset.user_test\n",
        "\n",
        "    # =========== усечённый\n",
        "    if len(dataset.User.keys()) > 5000:\n",
        "        users = random.sample(range(np.min(list(data_pcatsne.User.keys())), np.max(list(data_pcatsne.User.keys())) + 1), 5000)\n",
        "    else:\n",
        "        users = random.sample(range(np.min(list(data_pcatsne.User.keys())), np.max(list(data_pcatsne.User.keys())) + 1), 5000)\n",
        "    #users = range(1, usernum + 1)   ====== вариант для всех пользаков\n",
        "    \n",
        "\n",
        "    all_users_embeddings = []\n",
        "\n",
        "    for u in tqdm(users, ncols=70, leave=False, unit=\"b\"):\n",
        "\n",
        "        if len(train[u]) < 1 or len(test[u]) < 1:\n",
        "            continue\n",
        "\n",
        "        seq = np.zeros([model.seq_max_len], dtype=np.int32)\n",
        "        idx = model.seq_max_len - 1\n",
        "        seq[idx] = valid[u][0]\n",
        "        idx -= 1\n",
        "        for i in reversed(train[u]):\n",
        "            seq[idx] = i\n",
        "            idx -= 1\n",
        "            if idx == -1:\n",
        "                break\n",
        "        rated = set(train[u])\n",
        "        rated.add(0)\n",
        "        item_idx = [test[u][0]]\n",
        "        for _ in range(model.num_neg_test):\n",
        "            t = np.random.randint(1, itemnum + 1)\n",
        "            while t in rated:\n",
        "                t = np.random.randint(1, itemnum + 1)\n",
        "            item_idx.append(t)\n",
        "\n",
        "        inputs = {}\n",
        "        inputs[\"user\"] = np.expand_dims(np.array([u]), axis=-1)\n",
        "        inputs[\"input_seq\"] = np.array([seq])\n",
        "        inputs[\"candidate\"] = np.array([item_idx])\n",
        "\n",
        "        all_users_embeddings.append(model.get_user_embedding(inputs))\n",
        "\n",
        "    return all_users_embeddings"
      ],
      "metadata": {
        "id": "J0PwsrKqYDOU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = data_model\n",
        "val_epoch = 11\n",
        "\n",
        "num_steps = int(len(data_model.user_train) / batch_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-7\n",
        ")\n",
        "\n",
        "loss_function = model.loss_function\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "train_step_signature = [\n",
        "    {\n",
        "        \"users\": tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "        \"input_seq\": tf.TensorSpec(\n",
        "            shape=(None, model.seq_max_len), dtype=tf.int64\n",
        "        ),\n",
        "        \"positive\": tf.TensorSpec(\n",
        "            shape=(None, model.seq_max_len), dtype=tf.int64\n",
        "        ),\n",
        "        \"negative\": tf.TensorSpec(\n",
        "            shape=(None, model.seq_max_len), dtype=tf.int64\n",
        "        ),\n",
        "    },\n",
        "    tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    with tf.GradientTape() as tape:\n",
        "        pos_logits, neg_logits, loss_mask = model(inp, training=True)\n",
        "        loss = loss_function(pos_logits, neg_logits, loss_mask)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    #gradients = [(tf.clip_by_value(grad, -1.0, 1.0)) for grad in gradients]\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    return loss\n",
        "\n",
        "T = 0.0\n",
        "t0 = Timer()\n",
        "t0.start()\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    WATCH_LOSS = 0\n",
        "    counter = 0\n",
        "    step_loss = []\n",
        "    train_loss.reset_states()\n",
        "    for step in tqdm(\n",
        "        range(num_steps), total=num_steps, ncols=70, leave=False, unit=\"b\"\n",
        "    ):\n",
        "\n",
        "        u, seq, pos, neg = sampler.next_batch()\n",
        "\n",
        "        inputs, target = model.create_combined_dataset(u, seq, pos, neg)\n",
        "\n",
        "        loss = train_step(inputs, target)\n",
        "        step_loss.append(loss)\n",
        "        WATCH_LOSS += loss.numpy()\n",
        "        counter += 1\n",
        "    \n",
        "    print(f\"\\nIn {epoch} epoch the LOSS is {WATCH_LOSS/counter}\")\n",
        "    if epoch % val_epoch == 0:\n",
        "        t0.stop()\n",
        "        t1 = t0.interval\n",
        "        T += t1\n",
        "        print(\"Evaluating...\")\n",
        "        t_test = model.evaluate(data_model)\n",
        "        t_valid = model.evaluate_valid(data_model)\n",
        "        print(\n",
        "            f\"\\nepoch: {epoch}, time: {T}, valid (NDCG@10: {t_valid[0]}, HR@10: {t_valid[1]})\"\n",
        "        )\n",
        "        print(\n",
        "            f\"epoch: {epoch}, time: {T},  test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\"\n",
        "        )\n",
        "        t0.start()\n",
        "    # save model\n",
        "    model.save_weights(f'./drive/MyDrive/model_epoch_{epoch}_loss_{round(WATCH_LOSS/counter, 4)}') ### путь\n",
        "\n",
        "# last evaluation\n",
        "t_test = model.evaluate(data_model)\n",
        "print(f\"\\nepoch: {epoch}, test (NDCG@10: {t_test[0]}, HR@10: {t_test[1]})\")\n",
        "\n",
        "# get user's embeddings\n",
        "t0.stop()\n",
        "t1 = t0.interval\n",
        "T += t1\n",
        "print(\"Getting user's embeddings...\")\n",
        "user_embeddings = model.get_all_user_embeddings(data_pcatsne)\n",
        "print(\"Nice:)\")\n",
        "t0.start()"
      ],
      "metadata": {
        "id": "ARbrWxXkGepJ",
        "outputId": "b3d15aef-f6d8-4cef-b7e3-81b7a292a3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 1 epoch the LOSS is 1.0119078552626382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 2 epoch the LOSS is 0.5024754935471769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 3 epoch the LOSS is 0.28866985907379705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 4 epoch the LOSS is 0.20676055788397504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 5 epoch the LOSS is 0.16543507289182235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 6 epoch the LOSS is 0.13922726224509804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 7 epoch the LOSS is 0.12164467009846627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 8 epoch the LOSS is 0.10896730709780166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 9 epoch the LOSS is 0.09883668682599647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 10 epoch the LOSS is 0.09151290515246932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In 11 epoch the LOSS is 0.08532701729805257\n",
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 11, time: 1100.805421398, valid (NDCG@10: 0.03333333333333333, HR@10: 0.04)\n",
            "epoch: 11, time: 1100.805421398,  test (NDCG@10: 0.031065941916258222, HR@10: 0.05)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 11, test (NDCG@10: 0.03622195671295349, HR@10: 0.07)\n",
            "Getting user's embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-01244fcdc94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Getting user's embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0muser_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_user_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pcatsne\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nice:)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SASREC' object has no attribute 'get_all_user_embeddings'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get user's embeddings\n",
        "t0.stop()\n",
        "t1 = t0.interval\n",
        "T += t1\n",
        "print(\"Getting user's embeddings...\")\n",
        "user_embeddings = get_all_user_embeddings(model, data_pcatsne)\n",
        "print(\"Nice:)\")\n",
        "t0.start()"
      ],
      "metadata": {
        "id": "57oDzrbl39Bb",
        "outputId": "e37421f7-1873-4d98-bc14-aba898f1af68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting user's embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice:)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with Timer() as train_time:\n",
        "#    t_test, user_embeddings = model.train(dataS, sampler, num_epochs=num_epochs, batch_size=batch_size, lr=lr, val_epoch=2)\n",
        "user_embeddings.shape"
      ],
      "metadata": {
        "id": "CMmI6SQKC366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "56970ed1-f63a-4ca1-eb59-c38c58cb0efe"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b1bab2eab2a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#with Timer() as train_time:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#    t_test, user_embeddings = model.train(dataS, sampler, num_epochs=num_epochs, batch_size=batch_size, lr=lr, val_epoch=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muser_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_embeddings = np.array(user_embeddings)"
      ],
      "metadata": {
        "id": "u0Rf2KnZHdmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(user_embeddings), np.max(user_embeddings))"
      ],
      "metadata": {
        "id": "35aCMJvUbe3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Эмбеды пользователей "
      ],
      "metadata": {
        "id": "zG4goDgr_XLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "XFhFpH9H_wvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вариант для 2д, PCA\n",
        "pca = PCA(n_components=2, whiten=True) \n",
        "X_pca = pca.fit_transform(user_embeddings)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [15.00, 7.0]\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "fig = plt.figure()\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], marker='.', c='blueviolet')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TFxb7PuryegV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вариант для 3д, PCA\n",
        "pca = PCA(n_components=3, whiten=True) \n",
        "X_pca = pca.fit_transform(user_embeddings)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [7.00, 3.50]\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "z, x, y = X_pca[:,0], X_pca[:,1], X_pca[:,2]\n",
        "ax.scatter(x, y, z, c=z, alpha=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xKtuqo6OZUrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вариант для 2д, tsne\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(user_embeddings)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [15.00, 7.0]\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "fig = plt.figure()\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], marker='.', c='fuchsia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SeoxK7A-1nNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вариант для 3д, tsne\n",
        "tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(user_embeddings)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [7.00, 3.50]\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "z, x, y = tsne_results[:,0], tsne_results[:,1], tsne_results[:,2]\n",
        "ax.scatter(x, y, z, c=z, alpha=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A272hQBCf5DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ошибка с DNN"
      ],
      "metadata": {
        "id": "B9Lhd7HeSKxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJiuk9aKQUv8",
        "outputId": "5be91eac-14be-454c-a25c-3049aaa8256f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 18 not upgraded.\n",
            "Need to get 420 MB of archives.\n",
            "After this operation, 3,369 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.4.1.50-1+cuda11.6 [420 MB]\n",
            "Fetched 420 MB in 25s (16.7 MB/s)\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n",
            "(Reading database ... 155654 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.4.1.50-1+cuda11.6_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.4.1.50-1+cuda11.6) over (8.0.5.39-1+cuda11.1) ...\n",
            "Setting up libcudnn8 (8.4.1.50-1+cuda11.6) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export PATH=/usr/local/cuda-11.4/bin${PATH:+:${PATH}}\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64:$LD_LIBRARY_PATH\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.4/include:$LD_LIBRARY_PATH\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64"
      ],
      "metadata": {
        "id": "Go-ioB3LRBRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite-model-maker==0.4.0\n",
        "!pip uninstall -y tensorflow && pip install -q tensorflow==2.9.1\n",
        "!pip install pycocotools==2.0.4\n",
        "!pip install opencv-python-headless==4.6.0.66"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QO7HxZ1GRG4N",
        "outputId": "4ab2cc75-e2cc-4e0d-86c2-b816bf55a254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflite-model-maker==0.4.0\n",
            "  Downloading tflite_model_maker-0.4.0-py3-none-any.whl (642 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m642.1/642.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers==1.12 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (1.12)\n",
            "Requirement already satisfied: tensorflow-datasets>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (4.6.0)\n",
            "Collecting numba==0.53\n",
            "  Downloading numba-0.53.0-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib<3.5.0,>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (3.2.2)\n",
            "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (2.8.1+zzzcolab20220518083849)\n",
            "Collecting tensorflow-model-optimization>=0.5\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting neural-structured-learning>=1.3.1\n",
            "  Downloading neural_structured_learning-1.4.0-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.6/128.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflowjs>=2.4.0\n",
            "  Downloading tensorflowjs-3.19.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (7.1.2)\n",
            "Requirement already satisfied: lxml>=4.6.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (4.9.1)\n",
            "Collecting tensorflow-addons>=0.11.2\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scann>=1.2.6\n",
            "  Downloading scann-1.2.7-cp37-cp37m-manylinux_2_27_x86_64.whl (11.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tflite-support>=0.4.0\n",
            "  Downloading tflite_support-0.4.1-cp37-cp37m-manylinux2014_x86_64.whl (42.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (0.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (1.24.3)\n",
            "Collecting tf-models-official==2.3.0\n",
            "  Downloading tf_models_official-2.3.0-py2.py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (5.4.1)\n",
            "Requirement already satisfied: Cython>=0.29.13 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (0.29.32)\n",
            "Collecting sentencepiece>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: librosa==0.8.1 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (1.21.6)\n",
            "Collecting fire>=0.3.1\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tflite-model-maker==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker==0.4.0) (1.7.3)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker==0.4.0) (0.4.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker==0.4.0) (1.6.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker==0.4.0) (1.0.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker==0.4.0) (3.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker==0.4.0) (21.3)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker==0.4.0) (0.10.3.post1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker==0.4.0) (1.1.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->tflite-model-maker==0.4.0) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.53->tflite-model-maker==0.4.0) (57.4.0)\n",
            "Collecting llvmlite<0.37,>=0.36.0rc1\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker==0.4.0) (4.6.0.66)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker==0.4.0) (5.4.8)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker==0.4.0) (1.21.0)\n",
            "Collecting tf-slim>=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker==0.4.0) (0.5.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker==0.4.0) (1.3.5)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker==0.4.0) (1.5.12)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official==2.3.0->tflite-model-maker==0.4.0) (1.12.11)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.3.1->tflite-model-maker==0.4.0) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker==0.4.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker==0.4.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker==0.4.0) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.5.0,>=3.0.3->tflite-model-maker==0.4.0) (0.11.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from neural-structured-learning>=1.3.1->tflite-model-maker==0.4.0) (22.1.0)\n",
            "Collecting tensorflow>=2.6.0\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (0.26.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (1.47.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (3.17.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (14.0.6)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (4.1.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (0.4.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (0.2.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tflite-model-maker==0.4.0) (1.6.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.11.2->tflite-model-maker==0.4.0) (2.7.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (4.64.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (5.9.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (1.9.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (0.3.5.1)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (0.7.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (0.10.2)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.5->tflite-model-maker==0.4.0) (0.1.7)\n",
            "Collecting packaging>=20.0\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflowjs>=2.4.0\n",
            "  Downloading tensorflowjs-3.18.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.6.0\n",
            "  Downloading pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.3/213.3 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sounddevice>=0.4.4\n",
            "  Downloading sounddevice-0.4.4-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (0.37.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (0.0.4)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (1.35.0)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (1.31.6)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (3.0.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (0.4.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (1.5.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (2022.6.15)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (6.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22.0->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (2022.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1->tflite-model-maker==0.4.0) (1.4.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.1->tflite-model-maker==0.4.0) (3.1.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.7/dist-packages (from sounddevice>=0.4.4->tflite-support>=0.4.0->tflite-model-maker==0.4.0) (1.15.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (0.4.6)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (3.8.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets>=2.1.0->tflite-model-maker==0.4.0) (1.56.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->tflite-support>=0.4.0->tflite-model-maker==0.4.0) (2.21)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (4.12.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (1.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->tflite-model-maker==0.4.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.6.0->tflite-model-maker==0.4.0) (3.2.0)\n",
            "Building wheels for collected packages: fire, py-cpuinfo\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=712ecb0face0f6e7320f2e81f0905176bc6ac99cf6239536ced73d45f9fd4711\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=6de3cec36e59bc68f8067ca429529e00ec65fbcc6e68e309d6e07ec52bb7277b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "Successfully built fire py-cpuinfo\n",
            "Installing collected packages: sentencepiece, py-cpuinfo, keras, dataclasses, tf-slim, tensorflow-model-optimization, tensorflow-estimator, pybind11, protobuf, packaging, llvmlite, fire, tensorflow-addons, sounddevice, numba, neural-structured-learning, tflite-support, tensorboard, tensorflow, tf-models-official, tensorflowjs, scann, tflite-model-maker\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.0\n",
            "    Uninstalling llvmlite-0.39.0:\n",
            "      Successfully uninstalled llvmlite-0.39.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.0\n",
            "    Uninstalling numba-0.56.0:\n",
            "      Successfully uninstalled numba-0.56.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.1+zzzcolab20220518083849\n",
            "    Uninstalling tensorflow-2.8.1+zzzcolab20220518083849:\n",
            "      Successfully uninstalled tensorflow-2.8.1+zzzcolab20220518083849\n",
            "Successfully installed dataclasses-0.6 fire-0.4.0 keras-2.9.0 llvmlite-0.36.0 neural-structured-learning-1.4.0 numba-0.53.0 packaging-20.9 protobuf-3.19.4 py-cpuinfo-8.0.0 pybind11-2.10.0 scann-1.2.7 sentencepiece-0.1.97 sounddevice-0.4.4 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-addons-0.17.1 tensorflow-estimator-2.9.0 tensorflow-model-optimization-0.7.3 tensorflowjs-3.18.0 tf-models-official-2.3.0 tf-slim-1.1.0 tflite-model-maker-0.4.0 tflite-support-0.4.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses",
                  "google",
                  "keras",
                  "packaging",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.9.1\n",
            "Uninstalling tensorflow-2.9.1:\n",
            "  Successfully uninstalled tensorflow-2.9.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools==2.0.4 in /usr/local/lib/python3.7/dist-packages (2.0.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.4) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.4) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools==2.0.4) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0.4) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python-headless==4.6.0.66 in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.6.0.66) (1.21.6)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RlDHLxzeROEp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}